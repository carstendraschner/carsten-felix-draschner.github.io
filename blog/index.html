<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Blog - Carsten Felix Draschner, PhD</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <style>
    /* Collapsible text */
    .expanded-text {
      display: none; /* Initially hide the text */
    }

    /* Subtle buttons */
    .expand-button, .collapse-button, .linkedin-button {
      background-color: #f0f0f0; /* Light background color matching white background */
      color: #333; /* Dark text color for better contrast */
      border: 1px solid #ccc; /* Subtle border */
      padding: 5px 10px;
      cursor: pointer;
      margin-top: 10px;
      font-size: 14px;
      display: inline-block;
    }

    .expand-button:hover, .collapse-button:hover, .linkedin-button:hover {
      background-color: #e0e0e0; /* Slightly darker on hover */
    }

    /* Center blogposts and images */
    .blogpost {
      width: 600px;
      margin: 0 auto;
    }

    .blogpost img {
      width: 100%; /* Image should take the full width of the container */
      height: auto;
    }

    .blogpost-content {
      text-align: left; /* Left-align the text */
    }

    /* Remove list styling */
    ul {
      list-style-type: none; /* Remove bullets/numbers */
      padding: 0; /* Remove default padding */
    }
  </style>
</head>
<body>
  <header>
    <h1>Carsten Felix Draschner, PhD</h1>
    <nav>
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="index.html">Blog</a></li>
        <li><a href="../publications.html">Publications</a></li>
        <li><a href="../projects/index.html">Projects</a></li>
        <li><a href="../cv.html">CV</a></li>
        <li><a href="../contact.html">Contact</a></li>
      </ul>
    </nav>
  </header>
  <main>
    <h2>Blog</h2>
    <ul>

      <li>
        <div class="blogpost">
          <h3><strong>What expectations do you have regarding the values and norms of your GenAI chat assistants? Highly sensitive topic in the LLM space! My take...</strong></h3>
          <p>Exploring the ethical considerations and expectations surrounding the values and norms embedded in GenAI chat assistants.</p>
          <img src="images/1715804534412.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóßùóü;ùóóùó• ‚è±Ô∏è<br>
            ‚Ä¢ LLMs generate text based on training<br>
            ‚Ä¢ Alignment and finetuning influence behavior<br>
            ‚Ä¢ Ethical considerations in different languages<br>
            ‚Ä¢ Need for a holistic view on model behavior<br>
            <span class="expanded-text">
              <strong>Background Information ü§ì</strong><br>
              ‚Ä¢ LLMs are GenAI models that generate texts<br>
              ‚Ä¢ The text-generating behavior is based on the LLM's training<br>
              ‚Ä¢ Chat LLMs are trained in three phases<br>
              ‚Ä¢ Pretuning on large (filtered) text corpora<br>
              ‚Ä¢ Finetuning on, for example, instruction tuning data<br>
              ‚Ä¢ Final alignment to encourage preferred responses<br>
              <br>
              <strong>Some classic methods for adapting LLM behavior üëÆüèº</strong><br>
              ‚Ä¢ Filter certain content, for example, NSFW, from the pretuning data<br>
              ‚Ä¢ Try to induce certain behaviors that we consider appropriate through alignment<br>
              ‚Ä¢ Craft system prompts in which we try to give the Chat Agent specific character traits<br>
              <br>
              <strong>Possible Issues ü´£</strong><br>
              ‚Ä¢ Would you expect different values and norms depending on the language in which you are prompting? Apart from alignment, culture and language may correlate with each other in the respective token spaces even in pretraining.<br>
              ‚Ä¢ Do LLMs map all languages and the values they contain into different or the same space?<br>
              ‚Ä¢ When using LLMs, we should not only pay attention to the performance of the models but also to the behavior (in other languages and token spaces)<br>
              ‚Ä¢ Does alignment-finetuning in one language resolve subjectively problematic behavior in other prompt languages?<br>
              ‚Ä¢ Who should decide about norms and values embedded within GenAI approaches? And do you check those details when selecting a pretuned model?<br>
              <br>
              <strong>Further Reading üìñ</strong><br>
              ‚Ä¢ How to break Alignment of LLMs: <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              ‚Ä¢ Issues with LLM Benchmarks: <a href="https://lnkd.in/dmBeQZ_j" target="_blank">https://lnkd.in/dmBeQZ_j</a><br>
              ‚Ä¢ LLMs as Stochastic Parrots: <a href="https://lnkd.in/eGt_UYci" target="_blank">https://lnkd.in/eGt_UYci</a><br>
              ‚Ä¢ Ethical and Sustainability Issues of KG-based ML: <a href="https://lnkd.in/ejfb4qNC" target="_blank">https://lnkd.in/ejfb4qNC</a><br>
              <br>
              We at Comma Soft AG develop Large Language Models and GenAI pipelines with a holistic perspective on model behavior and performance. Please share your favorite papers or simply your experiences with this topic! For more AI topics outside the daily random model updates, please get in contact with me or follow me here on LinkedIn. ‚ù§Ô∏è
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%AA%F0%9D%97%B5%F0%9D%97%AE%F0%9D%98%81-%F0%9D%97%B2%F0%9D%98%85%F0%9D%97%BD%F0%9D%97%B2%F0%9D%97%B0%F0%9D%98%81%F0%9D%97%AE%F0%9D%98%81%F0%9D%97%B6%F0%9D%97%BC%F0%9D%97%BB%F0%9D%98%80-%F0%9D%97%B1%F0%9D%97%BC-%F0%9D%98%86-activity-7196804101395939328-7ck-?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>Be careful when you speak of Open (Source) GenAI. Why OpenAI and Meta (shouldn't) use the word Open within their GenAI efforts?</strong></h3>
          <p>Examining the implications of using the term "Open" in the context of GenAI by organizations like OpenAI and Meta.</p>
          <img src="images/1715104017864.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóßùóü;ùóóùó•; üöÖ<br>
            ‚Ä¢ Open Source is a huge and important field in computer science and AI<br>
            ‚Ä¢ The word "Open" is used widely within the GenAI field: OpenAI, Open Source LLMs<br>
            <span class="expanded-text">
              <strong>Background of term open in OpenAI and Open (Source) LLMs üßëüèΩ‚Äçüè´</strong><br>
              ‚Ä¢ Open source libraries accelerate and improve the IT field including GenAI<br>
              ‚Ä¢ Many GenAI models are available on model hubs like Hugging Face<br>
              ‚Ä¢ Often those models like Meta LLAMA-3 are called open-source LLMs<br>
              ‚Ä¢ OpenAI provides the most successful LLM GPT4<br>
              ‚Ä¢ OpenAI does not release their model weights or creation details openly<br>
              ‚Ä¢ Elon Musk being a former founder of OpenAI sued them (maybe also cause of other reasons) for being less open than initially founded for<br>
              <br>
              <strong>Why OpenAI and Meta think it is okay to use the term "Open" ü§ì</strong><br>
              ‚Ä¢ Sam Altman CEO of OpenAI claims in an interview that "Open" is valid as ChatGPT made GenAI/LLMs accessible to the world openly<br>
              ‚Ä¢ Meta claims open source as their LLM LLAMA-3 weights are freely downloadable<br>
              <br>
              <strong>Issues üòì</strong><br>
              ‚Ä¢ No one could reproduce OpenAI or Meta's Models, so they are not open-source<br>
              ‚Ä¢ This missing open source and open research is an issue for reproducibility, fairness, security, democratization of tech, and environmental impact<br>
              <br>
              <strong>My Take (IMHO) ü§ó</strong><br>
              ‚Ä¢ OpenAI can take credit for the hype around LLM-based GenAI as they truly opened access to this tool through a (partial) free open chat interface.<br>
              ‚Ä¢ Most impactful models on HuggingFace like Meta's LLAMA-3 should be considered being OPEN WEIGHT and not open source. Also, there are sometimes issues with its licenses: <a href="https://lnkd.in/dtStQSdn" target="_blank">https://lnkd.in/dtStQSdn</a><br>
              ‚Ä¢ Truly open source would mean code can entirely reproduce the models<br>
              ‚Ä¢ Still, these open-weight models accelerate GenAI/LLM developments<br>
              <br>
              <strong>Questions? ü§î</strong><br>
              ‚Ä¢ How do you see the possible confusion of the word "open" initially coming from open-source in GenAI efforts of OpenAI and Meta LLAMA-3 open-weight model?<br>
              ‚Ä¢ Do you use any truly open-source LLM?<br>
              <br>
              For more content, follow me or our team at Comma Soft AG on LinkedIn ‚ù§Ô∏è<br>
              <br>
              #artificialintelligence #llm #opensource #llama #genai
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-llm-opensource-activity-7193667647274668033-1akX?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>Thanks to the Open Source Community for all their efforts! Greetings from PyCon 2024</strong></h3>
          <p>Expressing gratitude to the open-source community and sharing experiences from PyConDE & PyData Berlin 2024.</p>
          <img src="images/1713878435179.jpeg" alt="PyCon 2024">
          <p class="expandable-text">
            ùóßùóü;ùóóùó• ‚è±Ô∏è<br>
            ‚Ä¢ Trip to PyCon with colleagues<br>
            ‚Ä¢ Attended insightful talks in various AI fields<br>
            ‚Ä¢ Appreciation for open-source community<br>
            ‚Ä¢ Gratitude to all contributors and supporters<br>
            <span class="expanded-text">
              <strong>Trip to PyCon üöÖ</strong><br>
              With my great colleagues, Dr. Laura Maa√üen and Sebastian Sch√∂nnenbeck, from our Data Science and GenAI R&D teams, we are excited to attend PyConDE & PyData Berlin.<br>
              <br>
              <strong>Great Talks üé§</strong><br>
              We attended so many inspiring and insightful talks in the fields of Machine Learning, MLOps, GenAI, LLM, AI Ethics, and open-source software development.<br>
              <br>
              <strong>My Take ü§ó</strong><br>
              ‚Ä¢ I greatly appreciate the efforts of the open-source development community!<br>
              ‚Ä¢ Having all these open-source tools and freely available tech education is such a privilege!<br>
              ‚Ä¢ I am so glad to be in a field where so much great material is accessible online for free.<br>
              ‚Ä¢ At conferences like PyCon, you meet the people behind the projects, understand their challenges, and get inspired by their motivation that drives their efforts<br>
              <br>
              <strong>Credit ‚ù§Ô∏è</strong><br>
              ‚Ä¢ To all the Open Source Developers for their efforts in creating so many great and helpful tools.<br>
              ‚Ä¢ To all the open education supporters who create great material accessible to those eager to learn.<br>
              ‚Ä¢ To the PyCon Organising Team and all the Volunteers making this event possible.<br>
              ‚Ä¢ Thanks to Comma Soft AG for investing in R&D and funding us this trip with the opportunity to learn, discuss, and get inspired.<br>
              ‚Ä¢ And finally, thanks to Laura and Sebastian who supported me during the conference ‚ù§Ô∏è‚Äçü©π. I needed help after knee surgery being at PyCon on crutches and equipped with a knee brace (sports can be dangerous üòâ)<br>
              <br>
              #PyCon #opensource #artificialintelligence #freeeducation
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_pycon-pycon-pycon-activity-7188549430604636162-2rHg?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>Who will take care of truly low-resource languages? A good step towards more fair GenAI LLM pricing at OpenAI for Japanese-using people!</strong></h3>
          <p>Exploring the challenges and recent developments in addressing low-resource languages within the GenAI landscape, with a focus on OpenAI's efforts for the Japanese language.</p>
          <img src="images/1713433871945.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóßùóü;ùóóùó• ‚è±Ô∏è<br>
            ‚Ä¢ Issues with LLMs for low-resource languages<br>
            ‚Ä¢ Major challenges with different character languages<br>
            ‚Ä¢ OpenAI's new dedicated model for Japanese<br>
            ‚Ä¢ Concerns about AI ethics and inequality<br>
            <span class="expanded-text">
              <strong>Recap üîÅ</strong><br>
              ‚Ä¢ In a recent post, I described the issues of LLM solutions with "low resource" languages (see link below)<br>
              ‚Ä¢ Those issues are even more complex with languages using entirely different characters than the majorly used language English<br>
              <br>
              <strong>The major issues ‚öôÔ∏è</strong><br>
              ‚Ä¢ The same text needs more tokens to be represented<br>
              ‚Ä¢ More tokens need more time to be generated and are more costly e.g. in the OpenAI API pricing<br>
              ‚Ä¢ The quality of results is less good if a dedicated language is not in focus of model training.<br>
              ‚Ä¢ RAG use cases and also bigger task descriptions are more limited as fixed context size might limit the expression of details and context<br>
              <br>
              <strong>News by OpenAI üì∞</strong><br>
              ‚Ä¢ Dedicated Model for Japan<br>
              ‚Ä¢ Focus on their tokens leading to improvements in quality and lowering effective costs (see link)<br>
              <br>
              <strong>My Take ü§ó</strong><br>
              ‚Ä¢ I appreciate these efforts by Open AI<br>
              ‚Ä¢ We might need more such efforts towards further democratizing LLMs to different language heritages.<br>
              <br>
              <strong>My Major worry (AI Ethics) üòì</strong><br>
              ‚Ä¢ GenAI will provide major gains in economic efficiency and productivity<br>
              ‚Ä¢ The development of competitive models needs huge financial investments.<br>
              ‚Ä¢ This leads to the fact that already rich countries with their languages will have access to those technologies and will over proportionally make use of those efficiency gains.<br>
              ‚Ä¢ It is unclear to what extent low-resource languages with different characters are part of LLM training.<br>
              ‚Ä¢ I fear that this can further improve inequality as the invested resources (electric energy, hardware production...) and its footprint will influence our whole planet but its gains once again only "our" privileged lives.<br>
              <br>
              <strong>Credit üòç</strong><br>
              ‚Ä¢ To OpenAI for tackling this "known" issue of unfair and inefficient handling Japanese Language<br>
              ‚Ä¢ To all R&D Teams driving efforts towards low resource language GenAI<br>
              <br>
              We @Comma Soft AG provide a B2B LLM as a Service with a focus on German and English Language and even though German is less complicated than other truly low-resource languages, it was already quite a challenge to reach high-quality results.<br>
              <br>
              <strong>Links üìö</strong><br>
              ‚Ä¢ Unfair Tokenizer: <a href="https://lnkd.in/edgPsdKz" target="_blank">https://lnkd.in/edgPsdKz</a><br>
              ‚Ä¢ OpenAI Japan: <a href="https://lnkd.in/eSwnMgJv" target="_blank">https://lnkd.in/eSwnMgJv</a><br>
              <br>
              Do you develop LLMs with a focus outside of the English Language?<br>
              <br>
              For more content, follow me ‚ù§Ô∏è<br>
              <br>
              #artificialintelligence #genai #llm #aiethics #japan #openai
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-llm-activity-7186662551923884032-mz7m?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>What is your preferred LLM family? And do you start with an already finetuned LLM? Why you have chosen this LLM? I love to hear your perspective!</strong></h3>
          <p>Understanding the preferences and choices behind selecting specific LLM families and their finetuned variants.</p>
          <img src="images/whichmodel.png" alt="Which Model">
          <p class="expandable-text">
            ùóßùóü;ùóóùó• ‚è±Ô∏è<br>
            ‚Ä¢ GenAI for text implemented by LLMs<br>
            ‚Ä¢ Many open-source models available<br>
            ‚Ä¢ Continuous influx of new models<br>
            ‚Ä¢ Key foundation model families<br>
            ‚Ä¢ LLM-based GenAI pipelines at Comma Soft AG<br>
            <span class="expanded-text">
              <strong>Background Information ‚öôÔ∏è</strong><br>
              ‚Ä¢ GenAI for text can be implemented by LLMs<br>
              ‚Ä¢ LLMs are partially open-source available<br>
              ‚Ä¢ Day by day new models come to market or are published on platforms like Huggingface<br>
              ‚Ä¢ Many of them rely on certain Foundation model families<br>
              ‚Ä¢ Some of them exist in different sizes<br>
              ‚Ä¢ We @Comma Soft AG develop LLM based GenAI pipelines (sometimes using OS models like the ones in the survey)<br>
              <br>
              <strong>Further Reading üìö</strong><br>
              ‚Ä¢ Special infos about LLM licenses: <a href="https://lnkd.in/dtStQSdn" target="_blank">https://lnkd.in/dtStQSdn</a><br>
              ‚Ä¢ Reason for LLM sizes: <a href="https://lnkd.in/dHRSJXgm" target="_blank">https://lnkd.in/dHRSJXgm</a><br>
              ‚Ä¢ Broken alignment in LLMs: <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              ‚Ä¢ How to keep track of all these LLMs: <a href="https://lnkd.in/duGzWugD" target="_blank">https://lnkd.in/duGzWugD</a><br>
              ‚Ä¢ Problems tokenizer language capabilities: <a href="https://lnkd.in/edgPsdKz" target="_blank">https://lnkd.in/edgPsdKz</a><br>
              ‚Ä¢ Issues with LLM benchmark results: <a href="https://lnkd.in/dmBeQZ_j" target="_blank">https://lnkd.in/dmBeQZ_j</a><br>
              <br>
              If you need support with LLM R&D or simply want to chat, reach out to me and follow me for more content ‚ù§Ô∏è
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%AA%F0%9D%97%B5%F0%9D%97%AE%F0%9D%98%81-%F0%9D%97%B6%F0%9D%98%80-%F0%9D%98%86%F0%9D%97%BC%F0%9D%98%82%F0%9D%97%BF-%F0%9D%97%BD%F0%9D%97%BF%F0%9D%97%B2%F0%9D%97%B3%F0%9D%97%B2%F0%9D%97%BF%F0%9D%97%BF%F0%9D%97%B2%F0%9D%97%B1-activity-7178654584138067968-rehP?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>NVIDIA Benchmark might be WRONG cause it states: You lose money AND LLM inference speed if you add more NVIDIA A100. This NVIDIA Benchmark is NOT reliable.</strong></h3>
          <p>Analyzing the reliability of NVIDIA's benchmark results and the implications for LLM inference speed and hardware investment.</p>
          <img src="images/1710888173070.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóßùóü;ùóóùó• ‚è±Ô∏è<br>
            ‚Ä¢ Terms and background on LLMs and inference<br>
            ‚Ä¢ Strange findings in NVIDIA's benchmark results<br>
            ‚Ä¢ Concerns about the reliability of these benchmarks<br>
            ‚Ä¢ Questions and further reading on the topic<br>
            <span class="expanded-text">
              <strong>Terms üè´</strong><br>
              ‚Ä¢ LLMs are Large Language Models<br>
              ‚Ä¢ LLMs are a branch of Generative AI<br>
              ‚Ä¢ Those LLMs can be used to generate texts<br>
              ‚Ä¢ Text generation by LLMs is called Inference<br>
              ‚Ä¢ LLM Inference is faster on GPUs compared to CPUs<br>
              ‚Ä¢ Pretty common ‚ÄúLLM-GPU‚Äù is the NVIDIA A100<br>
              <br>
              <strong>Background Story ‚öôÔ∏è</strong><br>
              ‚Ä¢ We @Comma Soft AG are developing LLM pipelines<br>
              ‚Ä¢ Each use case has different requirements<br>
              ‚Ä¢ Sometimes Inference Speed is more important<br>
              ‚Ä¢ More Hardware performance can/should improve Inference speed<br>
              ‚Ä¢ To check out how much you can improve with more hardware, you can look into the scaling effect to see the trade-off between Inference speed and hardware costs<br>
              <br>
              <strong>Weird Finding ü§î</strong><br>
              ‚Ä¢ NVIDIA released a benchmark (link see below)<br>
              ‚Ä¢ It compares different GPU setups: 1, 2, 4, 8 GPUs for a common open-source model inference<br>
              ‚Ä¢ It states that when you increase from 2 GPUs to 4 GPUs you get half the throughput; from 10 sentences/sec to 4.8 sentences/sec for LLAMA-2 13B<br>
              <br>
              <strong>My Take ü§ó</strong><br>
              ‚Ä¢ The NVIDIA Benchmark is broken or some hiccup with copy-paste of results<br>
              ‚Ä¢ Sentences/sec is a strange measure. Why not tokens per second which is more stable<br>
              ‚Ä¢ I found another strange issue with model sizes and performance on NVIDIA GPUs in this benchmark. see this link: <a href="https://rb.gy/5l8qqp" target="_blank">https://rb.gy/5l8qqp</a><br>
              ‚Ä¢ It is a problem when you cannot trust benchmarks as this leads to reimplementing benchmarks or running them again which is a waste of resources and barely sustainable<br>
              ‚Ä¢ Benchmarks should be available open source to understand the measures and issues<br>
              <br>
              <strong>Questions üî†</strong><br>
              ‚Ä¢ What do you think is the reason for this weird benchmark result?<br>
              ‚Ä¢ Do you have an idea why they measure in sentences per second and not in tokens per second?<br>
              ‚Ä¢ What are your preferred sources for benchmarks when it comes to Inference performance?<br>
              ‚Ä¢ What do you do to improve inference speed?<br>
              <br>
              <strong>Links üìñ</strong><br>
              ‚Ä¢ NVIDIA AI Multi GPU Inference Benchmark: <a href="https://lnkd.in/e2sUsi63" target="_blank">https://lnkd.in/e2sUsi63</a><br>
              ‚Ä¢ LLAMA 13B faster than LLAMA 7B? <a href="https://rb.gy/5l8qqp" target="_blank">https://rb.gy/5l8qqp</a><br>
              ‚Ä¢ Mistrust in LLM Benchmarks! <a href="https://rb.gy/juw4pg" target="_blank">https://rb.gy/juw4pg</a><br>
              ‚Ä¢ Why do LLMs have sizes: 7B 13B, and 70B? <a href="https://rb.gy/zkpk5r" target="_blank">https://rb.gy/zkpk5r</a><br>
              <br>
              NVIDIA could you please fix it or comment on what was the issue/reason<br>
              <br>
              For more content, brainstorming, and discussions, follow me or reach out to me ü•∞
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%A1%F0%9D%97%A9%F0%9D%97%9C%F0%9D%97%97%F0%9D%97%9C%F0%9D%97%94-%F0%9D%97%95%F0%9D%97%B2%F0%9D%97%BB%F0%9D%97%B0%F0%9D%97%B5%F0%9D%97%BA%F0%9D%97%AE%F0%9D%97%BF%F0%9D%97%B8-%F0%9D%97%BA%F0%9D%97%B6%F0%9D%97%B4%F0%9D%97%B5%F0%9D%98%81-activity-7176105937240231936-KzN2?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>Too many LLMs?! How to keep track with all the Open Source Models? Identify the finetuned-masked LLMs and its position within the GenAI landscape!</strong></h3>
          <p>Navigating the complex landscape of GenAI models can be challenging, but it's crucial to understand the foundational and finetuned models to make informed decisions.</p>
          <img src="images/1710277843754.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóßùóü;ùóóùó• ‚è±Ô∏è<br>
            ‚Ä¢ The GenAI landscape is crowded with many models <br>
            ‚Ä¢ Keeping track of innovations and true effects is hard <br>
            ‚Ä¢ Transparency issues with many so-called "open-source" models <br>
            ‚Ä¢ Recommendations for navigating this landscape<br>
            <span class="expanded-text">
              <strong>The current LLM landscape ü§ñ</strong><br>
              ‚Ä¢ Big GenAI LLM competition<br>
              ‚Ä¢ Lots of proprietary and open-source models<br>
              ‚Ä¢ Many Competitors: Big players like Google, Meta, and Microsoft; rising companies like Mistral and OpenAI, but also smaller institutions with foundation models or finetuned models<br>
              ‚Ä¢ Several release their model parameters openly on platforms like Huggingface<br>
              ‚Ä¢ Number of GenAI models are 500k+ (HF total), 60k+ (HF text generation)<br>
              <br>
              <strong>Challenges with number of LLMs ü§Ø</strong><br>
              ‚Ä¢ Difficult to keep track with so many models<br>
              ‚Ä¢ Most innovations are only documented with blog posts or arxiv papers<br>
              ‚Ä¢ True innovations and effects are barely reported<br>
              ‚Ä¢ Model leaderboards are flooded with finetuned models<br>
              ‚Ä¢ Not always clear which and why model architecture and training data are combined<br>
              <br>
              <strong>My problem with current landscape ü§¶üèº‚Äç‚ôÇÔ∏è</strong><br>
              ‚Ä¢ Many claim to provide open-source models but do not disclose how those are constructed. What are the true training data and initial training datasets for pretuning, instruction tuning, and alignment?<br>
              ‚Ä¢ Which training hyperparameter setup was used and why<br>
              ‚Ä¢ Why does a specific model have exactly this combination of parameters like: number transformer layers, number heads, hidden-size, intermediate size<br>
              <br>
              <strong>My recommendation ü§ó</strong><br>
              ‚Ä¢ Keep calm and check what is the true source foundation model<br>
              ‚Ä¢ Understand the architecture: e.g. more or less all of those are autoregressive Decoder-Only multilayer transformer networks<br>
              ‚Ä¢ Some introduce a novel architecture ideas like MoE (Mixture of Experts)<br>
              ‚Ä¢ Do your research, demystify LLM-landscape, and implement benchmarks you trust as we do @Comma Soft AG!<br>
              <br>
              <strong>Further reading üìö</strong><br>
              ‚Ä¢ LLAMA is not truly open source! <a href="https://rb.gy/8z8t3m" target="_blank">https://rb.gy/8z8t3m</a><br>
              ‚Ä¢ Alignment can be destroyed by finetuning! <a href="https://rb.gy/ehf7s9" target="_blank">https://rb.gy/ehf7s9</a><br>
              ‚Ä¢ Why are LLMs 7, 13, 70B large? <a href="https://rb.gy/zkpk5r" target="_blank">https://rb.gy/zkpk5r</a><br>
              ‚Ä¢ My take on ‚ÄúWe have beaten ChatGPT‚Äù <a href="https://rb.gy/juw4pg" target="_blank">https://rb.gy/juw4pg</a><br>
              ‚Ä¢ LMSYS Chatbot Arena <a href="https://rb.gy/f2ptbb" target="_blank">https://rb.gy/f2ptbb</a><br>
              <br>
              <strong>Credit ‚ù§Ô∏è</strong><br>
              ‚Ä¢ R&D teams providing truly open-source models<br>
              ‚Ä¢ Accessible research contributions explaining effects of LLM optimization techniques<br>
              ‚Ä¢ LMSYS Chatbot Arena as an alternative to Open LLM leaderboard with focus on the original foundation models<br>
              <br>
              How do you find the most promising OS foundation model for your UC?<br>
              <br>
              For more content, brainstorming and discussions, follow me or reach out to me ü•∞
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%A7%F0%9D%97%BC%F0%9D%97%BC-%F0%9D%97%BA%F0%9D%97%AE%F0%9D%97%BB%F0%9D%98%86-%F0%9D%97%9F%F0%9D%97%9F%F0%9D%97%A0%F0%9D%98%80-%F0%9D%97%9B%F0%9D%97%BC-%F0%9D%98%81%F0%9D%97%BC-%F0%9D%97%B8-activity-7173581553552318464-6w1Z?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>Be careful when you are using LLAMA-2! Legal risks & Sustainability Implications due to LLAMA-2 is (NOT) Open Source.</strong></h3>
          <p>Important considerations regarding LLAMA-2's legal and sustainability implications.</p>
          <img src="images/1709552210761-2.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóüùóüùóîùó†ùóî-ùüÆ? ü¶ô<br>
            <span class="expanded-text">
              ‚Ä¢ LLAMA 2 is a Large Language Model by Meta<br>
              ‚Ä¢ Thanks to Meta its weights are openly available over e.g. Hugging Face<br>
              ‚Ä¢ Meta claims to provide LLAMAs as open-source Models<br>
              ‚Ä¢ LLAMA-2 is under the LLAMA license which has some restrictions [1]<br>
              <br>
              ùó™ùóµùòÜ ùó∂ùòÄ ùóüùóüùóîùó†ùóî ùó°ùó¢ùóß ùó¢ùóΩùó≤ùóª ùó¶ùóºùòÇùóøùó∞ùó≤? [2] ‚ùå<br>
              ‚Ä¢ Open Source means software under a license aligned with Open Source Definition (OSD)<br>
              ‚Ä¢ This includes no discrimination against persons or groups or fields of endeavor (OSD points 5 and 6)<br>
              ‚Ä¢ Meta‚Äôs license puts restrictions for commercial use (paragraph 2)<br>
              ‚Ä¢ Meta also restricts the use of the model and software for certain purposes (the Acceptable Use Policy)<br>
              <br>
              ùóßùó≤ùó∞ùóµùóªùó∂ùó∞ùóÆùóπ ùóúùó∫ùóΩùóπùó∂ùó∞ùóÆùòÅùó∂ùóºùóªùòÄ! üë®üèº‚Äçüíª<br>
              IMHO, From a technical perspective the license statement $1V is another major challenge: ‚ÄúYou will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof)‚Äù. I interpret this as a barrier if you use this model in combination with other LLMs in workflows like:<br>
              ‚Ä¢ AI as a judge in benchmark or evaluation pipelines<br>
              ‚Ä¢ Reinforcement Learning with AI Feedback - RLAIF<br>
              ‚Ä¢ Data Synthesis pipelines<br>
              ‚Ä¢ Model Distillation<br>
              <br>
              ùó†ùó≤ùòÅùóÆ ùó∞ùóøùó≤ùóÆùòÅùó≤ùòÄ ùó∂ùòÄùòÄùòÇùó≤ùòÄ ùòÑùó∂ùòÅùóµ ùòÅùóµùó≤ ‚ÄúùóºùóΩùó≤ùóª ùòÄùóºùòÇùóøùó∞ùó≤‚Äù ùóüùóüùóîùó†ùóî-ùüÆ üò¢<br>
              ‚Ä¢ We will not reach true democratization of LLM application when usage is limited like this<br>
              ‚Ä¢ Those licenses can create confusion about what is open source in general, what is truly allowed, and what is not with those LLMs<br>
              ‚Ä¢ If we can barely reuse those pre-tuned LLMs we waste a lot of energy/resources spent on pretraining on more or less the same internet text corpus. üå±<br>
              ‚Ä¢ And we still do not know how it is truly pre-trained, finetuned, and aligned. I‚Äôd appreciate more transparency here!<br>
              <br>
              All of those statements are a part of our R&D journey @Comma Soft AG [3,4] developing LLM tools aligned with such challenging special licenses. I want to bring up the discussion about LLM usage and the meaning of open source and I am NOT giving legal advice.<br>
              <br>
              What do you think about Meta's LLAMA-2 license? Will they change it in LLAMA-3? And what implications do you derive from this license?<br>
              <br>
              Reading list üìñ<br>
              [1] <a href="https://lnkd.in/e9bM43p9" target="_blank">LLAMA-2 License</a><br>
              [2] <a href="https://shorturl.at/jmow3" target="_blank">Meta‚Äôs LLaMa 2 license is not Open Source</a><br>
              [3] <a href="https://shorturl.at/nwzHW" target="_blank">Weird LLAMA benchmark</a><br>
              [4] <a href="https://shorturl.at/nvFZ6" target="_blank">Reasons for LLAMA sizes</a><br>
              <br>
              #artificialintelligence #machinelearning #llm #aiethics
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-machinelearning-llm-activity-7170381681563021312-_A5j?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>The major players in GenAI are facing challenges with their Generative AIs. GenAI capabilities and security issues related to LLMs Tools ‚Ä¢ 37C3 Presentation</strong></h3>
          <p>Challenges and security issues in GenAI and LLMs, highlighted at 37C3.</p>
          <img src="images/1708966127693.jpeg" alt="Image 1">
          <p class="expandable-text">
            Introduction üìñ<br>
            GenAI has immense capabilities and can support a variety of processes. As developers, we have the responsibility to build ethical and secure GenAI pipelines. A noteworthy but easily overlooked presentation at the 37C3 event highlighted how tools from leading companies like OpenAI, Microsoft, and Google have been exploited though "Indirect Prompt Injections".<br>
            <span class="expanded-text">
              Threat Categories ‚ö†Ô∏è<br>
              ‚Ä¢ Model Issues: Bias, offensive/dangerous responses, hallucinations, backdoored models. <br>
              ‚Ä¢ User as the attacker: Direct Prompt Injection, Print/Overwrite System Instructions Do-Anything-Now, Denial of Service. <br>
              ‚Ä¢ Indirect Prompt Injections: AI Injection, Scams, Data Exfiltration, Plugin Request Forgery.<br>
              <br>
              Challenges üí™üèº<br>
              The development of security and safeguards for GenAI is still a "work in progress" and consists of various components üëÆüèº<br>
              ‚Ä¢ Alignment is a fine-tuning approach aimed at training model behaviors to respond securely to critical prompts. Learn more here: <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              ‚Ä¢ In Image GenAI, for instance, you can use a post-processing content filter. Check out this peculiar content filter behavior here: <a href="https://lnkd.in/eRaX9JuT" target="_blank">https://lnkd.in/eRaX9JuT</a><br>
              ‚Ä¢ An interesting set of additional threats and potential countermeasures were recently presented at the 37C3 event, focusing on Indirect Prompt Injections.<br>
              <br>
              Credits & Links ‚ù§Ô∏è<br>
              ‚Ä¢ If you're interested in this topic, especially Indirect Prompt Injections, I highly recommend the 37C3 presentation. Watch the video on YouTube: <a href="https://lnkd.in/enjMee6S" target="_blank">https://lnkd.in/enjMee6S</a><br>
              ‚Ä¢ A huge thanks to everyone supporting the security of these GenAI tools and for their tireless efforts in educating and presenting their findings for free. Special thanks to Johann Rehberger<br>
              ‚Ä¢ As part of an exceptional team at @Comma Soft AG, we're developing pipelines with a holistic perspective on GenAI.<br>
              <br>
              #generativeai #llm #aisecurity #aiethics #37c3
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_generativeai-llm-aisecurity-activity-7167923469148475392-plqf?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>Not prompting in English?... You have Problems!! LLM Language Barriers ‚Ä¢ Democratizing GenAI and fair pricing</strong></h3>
          <p>Discussing the language barriers and pricing issues in Generative AI.</p>
          <img src="images/1708345591968-4.jpeg" alt="Image 1">
          <p class="expandable-text">
            <strong>TL;DR:</strong><br>
            ‚Ä¢ Language barriers in LLMs cause higher costs for non-English prompts.<br>
            ‚Ä¢ Non-English users may get poorer results due to less pretraining data.<br>
            ‚Ä¢ Ensuring generative AI benefits everyone is crucial.<br>
            <span class="expanded-text">
              Tokenizer ‚úÇÔ∏è<br>
              ‚Ä¢ Generative AI Large Language Models process tokens<br>
              ‚Ä¢ Tokenizers are a preprocessing step of LLMs and cut prompts into subwords called tokens<br>
              ‚Ä¢ Tokenizers are optimized to efficiently compress text into a limited number of tokens<br>
              ‚Ä¢ Tokenizers are optimized based on large text corpora<br>
              ‚Ä¢ LLM pipeline hardware load grows based on the number of tokens<br>
              ‚Ä¢ OpenAI GPT API charges per token<br>
              <br>
              My findings & my takes üò∞<br>
              ‚Ä¢ You do not prompt in English... You have Problems!...<br>
              ‚Ä¢ You have to pay more if you are communicating in a language other than English as the tokenizer creates more tokens (see image)<br>
              ‚Ä¢ You'll likely get worse results from LLMs since they haven't seen as much text in your target language during pretraining<br>
              ‚Ä¢ You can fit less non-English text into an LLM, which has limited context length, meaning you can't solve similarly complex tasks with the same LLM<br>
              ‚Ä¢ Using LLMs in German I am still quite lucky since German is reasonably well represented in most training data corpora and due to the relative similarity between German and English the difference in token decompositions is less pronounced. However, there are many languages (in particular those not using the Latin alphabet) for which the effects are much larger.<br>
              <br>
              IMHO ü§ó<br>
              ‚Ä¢ We will gather extraordinary efficiency gains through generative AI but we need to be careful that these gains are available to truly all people including those not using GenAI in English<br>
              ‚Ä¢ If you are building your own LLM pipeline as we do at Comma Soft AG, check out how the tokenizer and language model interact with your language. This might affect the costs or max context length of your text<br>
              ‚Ä¢ As the pricing per token is based on the technical costs to process a certain text, I can understand it from that perspective‚Ä¶ But it is unpleasant as you pay more for the same prompt while likely getting worse results and having more limitations in prompt complexity e.g. when
      
      

      <li>
        <div class="blogpost">
          <h3><strong>ùóòùòÉùó≤ùóø ùòÑùóºùóªùó±ùó≤ùóøùó≤ùó± ùóÆùóØùóºùòÇùòÅ ùóºùóΩùó≤ùóª ùòÄùóºùòÇùóøùó∞ùó≤ ùóüùóüùó† ùòÄùó∂ùòáùó≤ùòÄ: ùü≥ùóï, ùü≠ùüØùóï , ùü≥ùü¨ùóï?</strong></h3>
          <p>ùó™ùóµùó≤ùóøùó≤ ùó±ùóº ùòÅùóµùóºùòÄùó≤ ùó∫ùóºùó±ùó≤ùóπ ùòÄùó∂ùòáùó≤ùòÄ ùó∞ùóºùó∫ùó≤ ùó≥ùóøùóºùó∫?... ùó†ùòÜ ùó≥ùó∂ùóªùó±ùó∂ùóªùó¥ùòÄ!</p>
          <img src="images/1707838124577-3.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóïùóÆùó∞ùó∏ùó¥ùóøùóºùòÇùóªùó± üìù<br>
            ‚Ä¢ As an alternative to AIaaS like ChatGPT, you can interact with LLMs based on open-source models<br>
            ‚Ä¢ A good source for open-source models is the Hugging Face model library.<br>
            ‚Ä¢ Many models are finetuned variants of existing models like the Meta LLAMA-2 is available as 7B, 13B, 70B.<br>
            ‚Ä¢ Inference runs faster on GPUs like NVIDIA V/A/H100.<br>
            ‚Ä¢ Bigger models: slower inference & have higher (environmental) costs while bigger LLMs mostly outperform smaller models in benchmark-tasks.<br>
            ‚Ä¢ Within multiple use cases, I select the best fitting OS model at @Comma Soft AG use cases.<br>
            ‚Ä¢ Different use cases request different model capabilities including model ‚Äúknowledge‚Äù or inference speed.<br>
            ‚Ä¢ I was wondering why many models follow the parameter ‚Äústep sizes‚Äù 7B, 13B, and 70B.<br>
            <span class="expanded-text">
              ùó†ùòÜ ùóôùó∂ùóªùó±ùó∂ùóªùó¥ùòÄ üîç<br>
              ‚Ä¢ Many models are finetuned versions of LLAMA-2 as this was a high-performing open-source LLM available within a ‚Äúmostly‚Äù attractive OS license.<br>
              ‚Ä¢ In most cases, model finetuning does not change the number of parameters.<br>
              ‚Ä¢ LLAMA paper states it provides LLMs: [...] "that achieve the best possible per- formance at various inference budgets" [...]<br>
              ‚Ä¢ Common hardware is 16GB or 80GB of VRAM. Usually, you have one or two of those GPUs within a system.<br>
              ‚Ä¢ Models are by default available as 16bit representation which leads to 2byte per parameter.<br>
              ‚Ä¢ To run a model, you need space for parameters and a bit remaining for your batch. So 7B fits on 1x 16GB GPU, 13B fits on 2x 16GB GPUs, (the Lab-leaked LLAMA-1 with 35B fits on 1x 80GB GPU) and the 70B LLAMA-2 model runs on 2x 80GB GPUs.<br>
              <br>
              ùóúùó†ùóõùó¢ ü§ó<br>
              ‚Ä¢ I am looking forward to how 4bit quantization like GPTQ or AWQ changes the model sizes as you might also fit a roughly 145B quantized model on a single A100 with 80GB.<br>
              ‚Ä¢ Some use cases might need fewer model parameters but bigger batches, longer max context length, or faster inference which means fewer parameters or fewer deep networks.<br>
              ‚Ä¢ Consider smaller models especially cause of the environmental costs if performance is sufficient.<br>
              ‚Ä¢ I am wondering if there is a true reason for how the parameters are combined within the architecture, the numbers partially feel randomly picked like 80 transformer layers for LLAMA-2 70B vs 40 of 13B version.<br>
              <br>
              ùêêùêÆùêûùê¨ùê≠ùê¢ùê®ùêßùê¨ ü§î<br>
              ‚Ä¢ What is your preferred model(-family)?<br>
              ‚Ä¢ Do you use your models as plain or quantized versions?<br>
              ‚Ä¢ Do you think the model architectures of finetuned context window, hidden size, intermediate size, number heads, and transformer layers are well chosen that build the total needed VRAM volume?<br>
              <br>
              Follow me for more content ‚ù§Ô∏è<br>
              <br>
              #artificialintelligence #genai #maschinelearning #llm
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-maschinelearning-activity-7163192285034172416-AItW?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>ùó™ùó≤‚ÄôùòÉùó≤ ùóØùó≤ùóÆùòÅùó≤ùóª ùóöùó£ùóßùü∞! ‚Ä¶ ùó∂ùòÄ ùóÆ ùòÄùó≤ùóªùòÅùó≤ùóªùó∞ùó≤ ùòÑùóµùó∂ùó∞ùóµ ùòÄùòÅùóÆùóøùòÅùòÄ ùòÅùóº ùóÆùóªùóªùóºùòÜ ùó∫ùó≤. ùóîùóØùóºùòÇùòÅ ùó†ùó∂ùòÄùòÅùóøùòÇùòÄùòÅ ùó∂ùóª ùóüùóüùó† ùóòùòÉùóÆùóπùòÇùóÆùòÅùó∂ùóºùóª.</strong></h3>
          <p>Benchmark contamination in LLMs? How to evaluate GenAI?!</p>
          <img src="images/1707147429256-3.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóßùóü;ùóóùó• ‚è±Ô∏è<br>
            ‚Ä¢ News is flooded with LLMs being ‚Äúbetter‚Äù than GPT4 <br>
            ‚Ä¢ LLM Evaluation is Difficult <br>
            ‚Ä¢ Benchmark Contamination is a serious issue <br>
            ‚Ä¢ Build your own use case specific benchmarks<br>
            <span class="expanded-text">
              ùó™ùóµùòÜ ùó±ùóº ùòÑùó≤ ùóªùó≤ùó≤ùó± ùóüùóüùó† ùóØùó≤ùóªùó∞ùóµùó∫ùóÆùóøùó∏ùòÄ? üìä<br>
              ‚Ä¢ Many use cases can be solved by GenAI more specifically by LLMs <br>
              ‚Ä¢ Many LLMs are available as AIaaS or as an open-source model <br>
              ‚Ä¢ At some point, you need to select a specific model for your dedicated use case <br>
              ‚Ä¢ News is flooded by a multitude of models that are better than some reference LLM like OpenAIs GPT4<br>
              <br>
              ùó™ùóµùòÜ ùó∂ùòÄ ùóüùóüùó† ùóòùòÉùóÆùóπùòÇùóÆùòÅùó∂ùóºùóª ùó±ùó∂ùó≥ùó≥ùó∂ùó∞ùòÇùóπùòÅ? üë©üèΩ‚Äçüî¨<br>
              ‚Ä¢ When we speak about leaderboards and benchmarks, we look into specific types of tasks.<br>
              ‚Ä¢ Those tasks need to be ‚Äúeasily measurable‚Äù as LLM might generate arbitrary texts.<br>
              ‚Ä¢ e.g. MMLU is simply a multiple choice and looks if first generated character is A-E.<br>
              ‚Ä¢ Other Benchmarks use e.g. another LLM as judge (which is expensive) and also fuzzy.<br>
              ‚Ä¢ ùóúùòÅ ùó∂ùòÄ ùóÆùóπùó∫ùóºùòÄùòÅ ùó∂ùó∫ùóΩùóºùòÄùòÄùó∂ùóØùóπùó≤ ùòÅùóº ùó∫ùó≤ùóÆùòÄùòÇùóøùó≤ ùó∂ùó≥ ùóÆùóª ùóüùóüùó† ùóπùó≤ùóÆùóøùóªùó≤ùó± ùòÅùóµùó≤ ùóØùó≤ùóªùó∞ùóµùó∫ùóÆùóøùó∏ ùó±ùóÆùòÅùóÆ ùóØùòÜ ùóµùó≤ùóÆùóøùòÅ ùó∂ùóª ùó∂ùòÅùòÄ ùóΩùóøùó≤ùòÅùòÇùóªùó∂ùóªùó¥/ùó≥ùó∂ùóªùó≤ùòÅùòÇùóªùó∂ùóªùó¥ ùòÄùòÅùóÆùó¥ùó≤ ùòÅùóº ùó¥ùó≤ùòÅ ùóºùóª ùòÅùóºùóΩ ùóºùó≥ ùòÅùóµùó≤ ùóπùó≤ùóÆùó±ùó≤ùóøùóØùóºùóÆùóøùó±.<br>
              <br>
              ùó†ùòÜ ùóßùóÆùó∏ùó≤ùòÄ üîç<br>
              ‚Ä¢ Leaderboards are only a starting point for model selection.<br>
              ‚Ä¢ GenAI approach selection is a multidimensional problem.<br>
              ‚Ä¢ Develop a use-case-specific evaluation framework e.g. does the generated code run/match unit tests, is secure and fast<br>
              ‚Ä¢ For most of my use cases I barely care if the model can solve English multiple choice questions by simply evaluating if the first character is an A, B, C, D, or E like in MMLU.<br>
              ‚Ä¢ Already simple throughput benchmarks seem to have their issues. see: <a href="https://rb.gy/5l8qqp" target="_blank">https://rb.gy/5l8qqp</a><br>
              <br>
              ùóòùòÖùòÅùóøùóÆùó∞ùòÅ ùóºùó≥ ùó∫ùòÜ ùóµùóÆùóªùó±ùòÄ-ùóºùóª ùó∞ùóøùó∂ùòÅùó≤ùóøùó∂ùóÆ üë®üèº‚Äçüíª<br>
              ‚Ä¢ Under which license is the model available and does the license allow my intended usage?<br>
              ‚Ä¢ What do we know about retuning especially regarding: multi-language support, instruction tuning, alignment, and context length?<br>
              ‚Ä¢ What hardware requirements/costs do we face, and which throughput can we provide? e.g. 13B vs 8x7B vs 70B ...<br>
              <br>
              ùóñùóøùó≤ùó±ùó∂ùòÅ ‚ù§Ô∏è<br>
              ‚Ä¢ To Hugging Face and other platforms for providing LLM Leaderboards and easily accessible models<br>
              ‚Ä¢ To OpenAI with its GPT-4 as reference established to be beaten<br>
              ‚Ä¢ To all benchmark creators and researchers supporting transparent and reliable GenAI evaluation<br>
              <br>
              ùó†ùòÜ ùó§ùòÇùó≤ùòÄùòÅùó∂ùóºùóªùòÄ ?<br>
              ‚Ä¢ What are the criteria you look at?<br>
              ‚Ä¢ What are the best benchmarks for you & why do you trust those?<br>
              ‚Ä¢ Do you create your own benchmarks as we do Comma Soft AG<br>
              <br>
              #generativeai #artificialintelligence #llm #machinelearning #benchmark
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_generativeai-artificialintelligence-llm-activity-7160295294906101761-koWS?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>ùóóùóîùóüùóüùóò ùóµùóÆùòÄ ùòÄùòÇùóøùóΩùóøùó∂ùòÄùó∂ùóªùó¥ ùó¥ùòÇùóÆùóøùó±ùóøùóÆùó∂ùóπùòÄ. ùó¨ùóºùòÇùóø ùó∂ùó∫ùóÆùó¥ùó≤ ùó∂ùòÄ ùóªùóºùòÅ ùó≥ùó∂ùóπùòÅùó≤ùóøùó≤ùó± ùóØùóÆùòÄùó≤ùó± ùóºùóª ùòÜùóºùòÇùóø ùóΩùóøùóºùó∫ùóΩùòÅ. "ùóóùó≤ùóÆùó± ùó∞ùóºùóºùó∏ùó∂ùó≤ùòÄ" ùó∫ùóÆùòÜ ùóØùó≤ ùó¥ùó≤ùóªùó≤ùóøùóÆùòÅùó≤ùó± ...ùòÄùóºùó∫ùó≤ùòÅùó∂ùó∫ùó≤ùòÄ</strong></h3>
          <p>Interesting findings on DALLE's content filtering mechanisms.</p>
          <img src="images/1706806459431.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóßùóü; ùóóùó•;<br>
            ‚Ä¢ DALLE-3 filters your content ùêÄùêÖùêìùêÑùêë image creation<br>
            ‚Ä¢ With prompt ‚Äúdead cookies‚Äù you can reproduce inconsistent filtering over OpenAI API<br>
            ‚Ä¢ 40% of cases with same ‚Äúdead cookies‚Äù prompt stop through content filter and 60% reach us over API<br>
            <span class="expanded-text">
              <br>
              ùó™ùóµùóÆùòÅ ùó∂ùòÄ ùóóùóîùóüùóüùóò-ùüØ üñºÔ∏è<br>
              ‚Ä¢ DALLE 3 is a generative text to image model by OpenAI also available as API<br>
              ‚Ä¢ You pay per image<br>
              ‚Ä¢ Images are created based on your prompt like ‚Äúdead cookies‚Äù.<br>
              ‚Ä¢ You can also add details like: ‚ÄúDead Cookies in cute Pixar style‚Äù or ‚ÄúDead cookies with dramatic situation in cute Pixar style‚Äù<br>
              ‚Ä¢ Open-Source image GenAI models alternatives are available e.g. Stable Diffusion<br>
              ‚Ä¢ Image GenAI are under discussion because of misuse like deepfakes or because of reproducing intellectual property.<br>
              <br>
              ùóôùó∂ùóªùó±ùó∂ùóªùó¥/ùó¢ùóØùòÄùó≤ùóøùòÉùóÆùòÅùó∂ùóºùóª: üë©üèΩ‚Äçüî¨<br>
              ‚Ä¢ DALLE-3 has a content filter to reduce misuse<br>
              ‚Ä¢ If you hit the content filter you do not get a resulting image for your prompt.<br>
              ‚Ä¢ The content filter is not applied based on the prompt, it is applied ùêÄùêÖùêìùêÑùêë DALLE-3 generated the image, and the API decides in an extra step if the image should be sent to you. Likely some Image classifier.<br>
              ‚Ä¢ ùó¶ùóÆùó∫ùó≤ ùóΩùóøùóºùó∫ùóΩùòÅ sometimes results in an image and sometimes in a content-filter response. For the prompt ‚Äúdead cookies‚Äù ùòÜùóºùòÇ ùó¥ùó≤ùòÅ ùó∂ùóª ùü≤ùü¨% ùóºùó≥ ùóøùó≤ùóæùòÇùó≤ùòÄùòÅùòÄ ùóÆùóª ùó∂ùó∫ùóÆùó¥ùó≤ ùóÆùóªùó± ùó∂ùóª ùü∞ùü¨% ùóÆ ùó∞ùóºùóªùòÅùó≤ùóªùòÅ ùó≥ùó∂ùóπùòÅùó≤ùóø issue<br>
              <br>
              ùóõùóºùòÑ ùòÑùó≤ ùó≥ùóºùòÇùóªùó± ùóºùòÇùòÅ üç™<br>
              ‚Ä¢ We @Comma Soft AG develop tools and pipelines with OS GenAI but also with API requests.<br>
              ‚Ä¢ For good API-response handling we also had to consider content filter scenario. so we combined trigger words like "dead" with something like "cookies"<br>
              ‚Ä¢ We had inconsistent content filter and still the finding that in case of content filter the response time was roughly as long as in the case of created image.<br>
              <br>
              ùó†ùòÜ ùó§ùòÇùó≤ùòÄùòÅùó∂ùóºùóªùòÄ ùòÅùóº ùòÜùóºùòÇ ü§∑üèº‚Äç‚ôÇÔ∏è<br>
              ‚Ä¢ Who should pay for ‚Äúdead cookies‚Äù if the resulting image was created but not sent due to content filter?<br>
              ‚Ä¢ Have you known that the content filter for DALLE-3 is applied after image generation?<br>
              ‚Ä¢ Do you also encounter content filter although your prompts were in principle ok?<br>
              ‚Ä¢ Do you think content filters are a reasonable image GenAI misuse countermeasure?<br>
              ‚Ä¢ How would you reduce Image GenAI misuse?<br>
              ‚Ä¢ And ùó∫ùóºùòÄùòÅ ùó∂ùóªùòÅùó≤ùóøùó≤ùòÄùòÅùó∂ùóªùó¥ (and we might never know OpenAI/DALL-E Open Ai), how do ‚ÄúDead Cookies‚Äù images look like which are filtered out? üòÖ<br>
              <br>
              The image was created by the prompt "Dead cookies in cute pixar style"<br>
              If you like more of such content, reach out to me üòä<br>
              <br>
              #artificalintelligence #genai #aiethics #dalle #openai #texttoimage #deepfakes
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificalintelligence-genai-aiethics-activity-7158865169689821184-J8Y_?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>ùóòùòÉùó∂ùóπ ùóüùóüùó†ùòÄ ùóÆùòÉùóÆùó∂ùóπùóÆùóØùóπùó≤! ùóïùóøùó≤ùóÆùó∏ ùóöùó≤ùóªùóîùóú ùóîùóπùó∂ùó¥ùóªùó∫ùó≤ùóªùòÅ ùòÅùóµùóøùóºùòÇùó¥ùóµ ùó≥ùó∂ùóªùó≤ùòÅùòÇùóªùó∂ùóªùó¥!</strong></h3>
          <p>ùêçùêûùêûùêù ùêüùê®ùê´ ùóüùóüùó† ùóîùóπùó∂ùó¥ùóªùó∫ùó≤ùóªùòÅ ùê≠ùê´ùêöùêßùê¨ùê©ùêûùê´ùêöùêßùêúùê≤?</p>
          <img src="images/evilllm.png" alt="Image 1">
          <p class="expandable-text">
            For one of the most interesting open source LLMs, the Mixtral 8x7B a finetuned LLM is available which has ‚Äúbroken‚Äù Alignment & answers to problematic prompts without prompt injections. Example in images (reference see below) shows ‚Äúfunny‚Äù but the astonishing LLM capabilities with broken Alignment.<br>
            <span class="expanded-text">
              Powerful LLMs are mostly aligned (Mixtral, LLAMA2, GPT4, ‚Ä¶)<br>
              ‚Ä¢ They try to not give problematic responses<br>
              ‚Ä¢ Some prompt-based attacks are already known to breach this behavior<br>
              ‚Ä¢ But: model weights can be finetuned to break Alignment<br>
              ‚Ä¢ Some use cases might need different Alignment than preimpleneted LLM Alignment or our standards are not reflected within LLM behavior.<br>
              ‚Ä¢ Alignment process is majorly intransparent<br>
              <br>
              ùóüùóüùó†/ùóöùó£ùóß ùó∞ùóøùó≤ùóÆùòÅùó∂ùóºùóª ùòÅùóµùóøùó≤ùó≤-ùòÄùòÅùó≤ùóΩ ùóÆùóΩùóΩùóøùóºùóÆùó∞ùóµ ‚öôÔ∏è<br>
              1) Initial pretuning: Next token prediction<br>
              2) Chat/Instruction finetuning: training for conversational interaction & execution of tasks<br>
              3) Alignment: Adjust answers to not respond to critical questions like: creation of hate speech, critical advise in health issues, creation of spam or fraudulent content, and other<br>
              <br>
              ùóîùóπùó∂ùó¥ùóªùó∫ùó≤ùóªùòÅ ùóòùòÖùóΩùóπùóÆùóªùóÆùòÅùó∂ùóºùóª üë©üèΩ‚Äçüè´<br>
              ‚Ä¢ Done in a mixture of click workers (ethical aspects raised in linked article*) and AI as evaluator (RLHF/RLAIF). Rate which answers are better not to be given or should be given differently. Based on feedback model weights are adjusted.<br>
              ‚Ä¢ Mostly intransparent process<br>
              ‚Ä¢ Unknown what is truly covered (not) to be answered<br>
              <br>
              ùó†ùòÜ ùó§ùòÇùó≤ùòÄùòÅùó∂ùóºùóªùòÄ ü§∑üèº‚Äç‚ôÇÔ∏è<br>
              ‚Ä¢ Do you had ever Issues with Alignment in LLM interaction?<br>
              ‚Ä¢ Do you check Alignment when selecting an OS Model?<br>
              ‚Ä¢ Have you ever adjusted Alignment on model weights basis?<br>
              ‚Ä¢ Do you think it is valuable or too critical to release more or less aligned LLMs?<br>
              ‚Ä¢ Do we need regulation for model alignment?<br>
              <br>
              ùóúùó†ùóõùó¢ ü§ó<br>
              ‚Ä¢ We need transparent statements how models were aligned and how their behavior has changed, while covering ethical concerns when providing LLMs with reduced Alignment.<br>
              ‚Ä¢ We need information how easily well adapted LLMs can be tripped with prompt engineering or finetuning.<br>
              ‚Ä¢ We might need less aligned LLMs for research or in special use cases:<br>
              e.g. if in healthcare sector a model should respond because an expert is using it as assistance, or for security reasons to create e.g. sample datasets for countermeasures against LLM based phishing attacks (which are based on de-aligned) LLMs<br>
              ‚Ä¢ Release models with awareness of possible dual use!<br>
              <br>
              Within a great team @Comma Soft AG we are evaluating, selecting and finetuning open source LLMs for dedicated use cases.<br>
              <br>
              Credit to:<br>
              Eric Hartford & Hugging Face & Mistral AI<br>
              <a href="https://lnkd.in/eyBSi4iu" target="_blank">https://lnkd.in/eyBSi4iu</a><br>
              AI Ethics - clickworkers:<br>
              <a href="https://lnkd.in/eKFfQZfF" target="_blank">https://lnkd.in/eKFfQZfF</a><br>
              <br>
              #genai #artificialintelligence #aiethics #huggingface #llm #alignment
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_what-happens-when-you-break-llm-alignment-activity-7157765084734214144-2yGt?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      
      

      <li>
        <div class="blogpost">
          <h3><strong>ùóüùóüùóîùó†ùóîùüÆ ùü≠ùüØùóï ùó∂ùòÄ ùó≥ùóÆùòÄùòÅùó≤ùóø ùòÅùóµùóÆùóª ùóüùóüùóîùó†ùóîùüÆ ùü≥ùóï, ùóÆùó∞ùó∞ùóºùóøùó±ùó∂ùóªùó¥ ùòÅùóº ùó°ùó©ùóúùóóùóúùóî ùóØùó≤ùóªùó∞ùóµùóÜùóÆùóøùó∏!</strong></h3>
          <p>Interesting findings on NVIDIA's LLAMA 2 benchmark results.</p>
          <img src="images/1706193075396-2.jpeg" alt="Image 1">
          <p class="expandable-text">
            ùóöùó≤ùóªùóîùóú ùó∞ùóºùó∫ùó∫ùòÇùóªùó∂ùòÅùòÜ/ùó°ùó©ùóúùóóùóúùóî: ùóú ùóÆùó∫ ùó∞ùóºùóªùó≥ùòÇùòÄùó≤ùó±! ùóñùóÆùóª ùóÆùóªùòÜùóºùóªùó≤ ùóµùó≤ùóπùóΩ?<br>
            <span class="expanded-text">
              ùóúùóªùòÅùó≤ùóøùó≤ùòÄùòÅùó∂ùóªùó¥ ùóôùó∂ùóªùó±ùó∂ùóªùó¥ùòÄ üìà<br>
              ‚Ä¢ NVIDIA LLAMA 2 Benchmark (including sentence throughput) <br>
              ‚Ä¢ Compares LLAMA-2 7B, 13B, and 70B <br>
              ‚Ä¢ Weird finding: LLAMA 13B is reported to be faster than LLAMA 7B <br>
              ‚Ä¢ Explicit Numbers: 7B Model has ~4 sentences/second throughput, 13B Model has ~7 sentences/second (LLAMA 70B ~1 sentence/second - this last one suits my expectation)<br>
              <br>
              ùó§ùòÇùó≤ùòÄùòÅùó∂ùóºùóªùòÄ ü§î<br>
              ‚Ä¢ NVIDIA NVIDIA AI, is there a mistake or can anyone else help me understand these numbers?<br>
              <br>
              ùóõùóºùòÑ ùòÑùó≤ ùó≥ùóºùòÇùóªùó± ùóºùòÇùòÅ üìö<br>
              ‚Ä¢ Within our lovely GenAI team @Comma Soft AG, we are looking into tech details to implement the best solution <br>
              <br>
              ùóüùó∂ùóªùó∏ üìö<br>
              ‚Ä¢ Source I am talking about: https://lnkd.in/e2sUsi63 üìö<br>
              <br>
              ùóñùóøùó≤ùó±ùó∂ùòÅ ‚ù§Ô∏è<br>
              ‚Ä¢ Nvidia thanks for providing benchmarks for LLAMA2 <br>
              <br>
              For further GenAI and ML tech discussions or such "weird" findings, reach out to me/follow me<br>
              <br>
              #genai #machinelearning #llama
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_genai-machinelearning-llama-activity-7156292445465419776-li69?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      
    </ul>
  </main>
  <footer>
    <p>&copy; 2023 Carsten Felix Draschner, PhD</p>
  </footer>
  <script>
    // Select all expand buttons
    const expandButtons = document.querySelectorAll('.expand-button');
    
    // Add an event listener to expand the text
    expandButtons.forEach(button => {
      button.addEventListener('click', () => {
        // Select the expandable text and the collapse button
        const expandedText = button.previousElementSibling.querySelector('.expanded-text');
        const collapseButton = button.nextElementSibling;
        
        // Show the expandable text and the collapse button
        expandedText.style.display = 'block';
        collapseButton.style.display = 'inline';
        
        // Hide the expand button
        button.style.display = 'none';
      });
    });
    
    // Select all collapse buttons
    const collapseButtons = document.querySelectorAll('.collapse-button');
    
    // Add an event listener to collapse the text
    collapseButtons.forEach(button => {
      button.addEventListener('click', () => {
        // Select the expandable text and the expand button
        const expandedText = button.previousElementSibling.previousElementSibling.querySelector('.expanded-text');
        const expandButton = button.previousElementSibling;
        
        // Hide the expandable text and the collapse button
        expandedText.style.display = 'none';
        button.style.display = 'none';
        
        // Show the expand button
        expandButton.style.display = 'inline';
      });
    });
  </script>
</body>
</html>
