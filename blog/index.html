<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Blog - Carsten Felix Draschner, PhD</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <style>
    /* Collapsible text */
    .expanded-text {
      display: none; /* Initially hide the text */
    }

    /* Subtle buttons */
    .expand-button, .collapse-button, .linkedin-button {
      background-color: #f0f0f0; /* Light background color matching white background */
      color: #333; /* Dark text color for better contrast */
      border: 1px solid #ccc; /* Subtle border */
      padding: 5px 10px;
      cursor: pointer;
      margin-top: 10px;
      font-size: 14px;
      display: inline-block;
    }

    .expand-button:hover, .collapse-button:hover, .linkedin-button:hover {
      background-color: #e0e0e0; /* Slightly darker on hover */
    }

    /* Center blogposts and images */
    .blogpost {
      width: 600px;
      margin: 0 auto;
    }

    .blogpost img {
      width: 100%; /* Image should take the full width of the container */
      height: auto;
    }

    .blogpost-content {
      text-align: left; /* Left-align the text */
    }

    /* Remove list styling */
    ul {
      list-style-type: none; /* Remove bullets/numbers */
      padding: 0; /* Remove default padding */
    }
  </style>
</head>
<body>
  <header>
    <h1>Carsten Felix Draschner, PhD</h1>
    <nav>
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="index.html">Blog</a></li>
        <li><a href="../publications.html">Publications</a></li>
        <li><a href="../projects/index.html">Projects</a></li>
        <li><a href="../cv.html">CV</a></li>
        <li><a href="../contact.html">Contact</a></li>
      </ul>
    </nav>
  </header>
  <main>
    <h2>Blog</h2>
    <ul>

      <li>
        <div class="blogpost">
          <h3><strong>ğ—˜ğ˜ƒğ—²ğ—¿ ğ˜„ğ—¼ğ—»ğ—±ğ—²ğ—¿ğ—²ğ—± ğ—®ğ—¯ğ—¼ğ˜‚ğ˜ ğ—¼ğ—½ğ—²ğ—» ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ—  ğ˜€ğ—¶ğ˜‡ğ—²ğ˜€: ğŸ³ğ—•, ğŸ­ğŸ¯ğ—• , ğŸ³ğŸ¬ğ—•?</strong></h3>
          <p>ğ—ªğ—µğ—²ğ—¿ğ—² ğ—±ğ—¼ ğ˜ğ—µğ—¼ğ˜€ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ˜€ğ—¶ğ˜‡ğ—²ğ˜€ ğ—°ğ—¼ğ—ºğ—² ğ—³ğ—¿ğ—¼ğ—º?... ğ— ğ˜† ğ—³ğ—¶ğ—»ğ—±ğ—¶ğ—»ğ—´ğ˜€!</p>
          <img src="images/1707838124577-3.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—•ğ—®ğ—°ğ—¸ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğŸ“<br>
            â€¢ As an alternative to AIaaS like ChatGPT, you can interact with LLMs based on open-source models<br>
            â€¢ A good source for open-source models is the Hugging Face model library.<br>
            â€¢ Many models are finetuned variants of existing models like the Meta LLAMA-2 is available as 7B, 13B, 70B.<br>
            â€¢ Inference runs faster on GPUs like NVIDIA V/A/H100.<br>
            â€¢ Bigger models: slower inference & have higher (environmental) costs while bigger LLMs mostly outperform smaller models in benchmark-tasks.<br>
            â€¢ Within multiple use cases, I select the best fitting OS model at @Comma Soft AG use cases.<br>
            â€¢ Different use cases request different model capabilities including model â€œknowledgeâ€ or inference speed.<br>
            â€¢ I was wondering why many models follow the parameter â€œstep sizesâ€ 7B, 13B, and 70B.<br>
            <span class="expanded-text">
              ğ— ğ˜† ğ—™ğ—¶ğ—»ğ—±ğ—¶ğ—»ğ—´ğ˜€ ğŸ”<br>
              â€¢ Many models are finetuned versions of LLAMA-2 as this was a high-performing open-source LLM available within a â€œmostlyâ€ attractive OS license.<br>
              â€¢ In most cases, model finetuning does not change the number of parameters.<br>
              â€¢ LLAMA paper states it provides LLMs: [...] "that achieve the best possible per- formance at various inference budgets" [...]<br>
              â€¢ Common hardware is 16GB or 80GB of VRAM. Usually, you have one or two of those GPUs within a system.<br>
              â€¢ Models are by default available as 16bit representation which leads to 2byte per parameter.<br>
              â€¢ To run a model, you need space for parameters and a bit remaining for your batch. So 7B fits on 1x 16GB GPU, 13B fits on 2x 16GB GPUs, (the Lab-leaked LLAMA-1 with 35B fits on 1x 80GB GPU) and the 70B LLAMA-2 model runs on 2x 80GB GPUs.<br>
              <br>
              ğ—œğ— ğ—›ğ—¢ ğŸ¤—<br>
              â€¢ I am looking forward to how 4bit quantization like GPTQ or AWQ changes the model sizes as you might also fit a roughly 145B quantized model on a single A100 with 80GB.<br>
              â€¢ Some use cases might need fewer model parameters but bigger batches, longer max context length, or faster inference which means fewer parameters or fewer deep networks.<br>
              â€¢ Consider smaller models especially cause of the environmental costs if performance is sufficient.<br>
              â€¢ I am wondering if there is a true reason for how the parameters are combined within the architecture, the numbers partially feel randomly picked like 80 transformer layers for LLAMA-2 70B vs 40 of 13B version.<br>
              <br>
              ğğ®ğğ¬ğ­ğ¢ğ¨ğ§ğ¬ ğŸ¤”<br>
              â€¢ What is your preferred model(-family)?<br>
              â€¢ Do you use your models as plain or quantized versions?<br>
              â€¢ Do you think the model architectures of finetuned context window, hidden size, intermediate size, number heads, and transformer layers are well chosen that build the total needed VRAM volume?<br>
              <br>
              Follow me for more content â¤ï¸<br>
              <br>
              #artificialintelligence #genai #maschinelearning #llm
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-maschinelearning-activity-7163192285034172416-AItW?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>ğ—ªğ—²â€™ğ˜ƒğ—² ğ—¯ğ—²ğ—®ğ˜ğ—²ğ—» ğ—šğ—£ğ—§ğŸ°! â€¦ ğ—¶ğ˜€ ğ—® ğ˜€ğ—²ğ—»ğ˜ğ—²ğ—»ğ—°ğ—² ğ˜„ğ—µğ—¶ğ—°ğ—µ ğ˜€ğ˜ğ—®ğ—¿ğ˜ğ˜€ ğ˜ğ—¼ ğ—®ğ—»ğ—»ğ—¼ğ˜† ğ—ºğ—². ğ—”ğ—¯ğ—¼ğ˜‚ğ˜ ğ— ğ—¶ğ˜€ğ˜ğ—¿ğ˜‚ğ˜€ğ˜ ğ—¶ğ—» ğ—Ÿğ—Ÿğ—  ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—».</strong></h3>
          <p>Benchmark contamination in LLMs? How to evaluate GenAI?!</p>
          <img src="images/1707147429256-3.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ News is flooded with LLMs being â€œbetterâ€ than GPT4 <br>
            â€¢ LLM Evaluation is Difficult <br>
            â€¢ Benchmark Contamination is a serious issue <br>
            â€¢ Build your own use case specific benchmarks<br>
            <span class="expanded-text">
              ğ—ªğ—µğ˜† ğ—±ğ—¼ ğ˜„ğ—² ğ—»ğ—²ğ—²ğ—± ğ—Ÿğ—Ÿğ—  ğ—¯ğ—²ğ—»ğ—°ğ—µğ—ºğ—®ğ—¿ğ—¸ğ˜€? ğŸ“Š<br>
              â€¢ Many use cases can be solved by GenAI more specifically by LLMs <br>
              â€¢ Many LLMs are available as AIaaS or as an open-source model <br>
              â€¢ At some point, you need to select a specific model for your dedicated use case <br>
              â€¢ News is flooded by a multitude of models that are better than some reference LLM like OpenAIs GPT4<br>
              <br>
              ğ—ªğ—µğ˜† ğ—¶ğ˜€ ğ—Ÿğ—Ÿğ—  ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—±ğ—¶ğ—³ğ—³ğ—¶ğ—°ğ˜‚ğ—¹ğ˜? ğŸ‘©ğŸ½â€ğŸ”¬<br>
              â€¢ When we speak about leaderboards and benchmarks, we look into specific types of tasks.<br>
              â€¢ Those tasks need to be â€œeasily measurableâ€ as LLM might generate arbitrary texts.<br>
              â€¢ e.g. MMLU is simply a multiple choice and looks if first generated character is A-E.<br>
              â€¢ Other Benchmarks use e.g. another LLM as judge (which is expensive) and also fuzzy.<br>
              â€¢ ğ—œğ˜ ğ—¶ğ˜€ ğ—®ğ—¹ğ—ºğ—¼ğ˜€ğ˜ ğ—¶ğ—ºğ—½ğ—¼ğ˜€ğ˜€ğ—¶ğ—¯ğ—¹ğ—² ğ˜ğ—¼ ğ—ºğ—²ğ—®ğ˜€ğ˜‚ğ—¿ğ—² ğ—¶ğ—³ ğ—®ğ—» ğ—Ÿğ—Ÿğ—  ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ—²ğ—± ğ˜ğ—µğ—² ğ—¯ğ—²ğ—»ğ—°ğ—µğ—ºğ—®ğ—¿ğ—¸ ğ—±ğ—®ğ˜ğ—® ğ—¯ğ˜† ğ—µğ—²ğ—®ğ—¿ğ˜ ğ—¶ğ—» ğ—¶ğ˜ğ˜€ ğ—½ğ—¿ğ—²ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´/ğ—³ğ—¶ğ—»ğ—²ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ˜€ğ˜ğ—®ğ—´ğ—² ğ˜ğ—¼ ğ—´ğ—²ğ˜ ğ—¼ğ—» ğ˜ğ—¼ğ—½ ğ—¼ğ—³ ğ˜ğ—µğ—² ğ—¹ğ—²ğ—®ğ—±ğ—²ğ—¿ğ—¯ğ—¼ğ—®ğ—¿ğ—±.<br>
              <br>
              ğ— ğ˜† ğ—§ğ—®ğ—¸ğ—²ğ˜€ ğŸ”<br>
              â€¢ Leaderboards are only a starting point for model selection.<br>
              â€¢ GenAI approach selection is a multidimensional problem.<br>
              â€¢ Develop a use-case-specific evaluation framework e.g. does the generated code run/match unit tests, is secure and fast<br>
              â€¢ For most of my use cases I barely care if the model can solve English multiple choice questions by simply evaluating if the first character is an A, B, C, D, or E like in MMLU.<br>
              â€¢ Already simple throughput benchmarks seem to have their issues. see: <a href="https://rb.gy/5l8qqp" target="_blank">https://rb.gy/5l8qqp</a><br>
              <br>
              ğ—˜ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ ğ—¼ğ—³ ğ—ºğ˜† ğ—µğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—°ğ—¿ğ—¶ğ˜ğ—²ğ—¿ğ—¶ğ—® ğŸ‘¨ğŸ¼â€ğŸ’»<br>
              â€¢ Under which license is the model available and does the license allow my intended usage?<br>
              â€¢ What do we know about retuning especially regarding: multi-language support, instruction tuning, alignment, and context length?<br>
              â€¢ What hardware requirements/costs do we face, and which throughput can we provide? e.g. 13B vs 8x7B vs 70B ...<br>
              <br>
              ğ—–ğ—¿ğ—²ğ—±ğ—¶ğ˜ â¤ï¸<br>
              â€¢ To Hugging Face and other platforms for providing LLM Leaderboards and easily accessible models<br>
              â€¢ To OpenAI with its GPT-4 as reference established to be beaten<br>
              â€¢ To all benchmark creators and researchers supporting transparent and reliable GenAI evaluation<br>
              <br>
              ğ— ğ˜† ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ?<br>
              â€¢ What are the criteria you look at?<br>
              â€¢ What are the best benchmarks for you & why do you trust those?<br>
              â€¢ Do you create your own benchmarks as we do Comma Soft AG<br>
              <br>
              #generativeai #artificialintelligence #llm #machinelearning #benchmark
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_generativeai-artificialintelligence-llm-activity-7160295294906101761-koWS?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>ğ——ğ—”ğ—Ÿğ—Ÿğ—˜ ğ—µğ—®ğ˜€ ğ˜€ğ˜‚ğ—¿ğ—½ğ—¿ğ—¶ğ˜€ğ—¶ğ—»ğ—´ ğ—´ğ˜‚ğ—®ğ—¿ğ—±ğ—¿ğ—®ğ—¶ğ—¹ğ˜€. ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¶ğ—ºğ—®ğ—´ğ—² ğ—¶ğ˜€ ğ—»ğ—¼ğ˜ ğ—³ğ—¶ğ—¹ğ˜ğ—²ğ—¿ğ—²ğ—± ğ—¯ğ—®ğ˜€ğ—²ğ—± ğ—¼ğ—» ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜. "ğ——ğ—²ğ—®ğ—± ğ—°ğ—¼ğ—¼ğ—¸ğ—¶ğ—²ğ˜€" ğ—ºğ—®ğ˜† ğ—¯ğ—² ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—²ğ—± ...ğ˜€ğ—¼ğ—ºğ—²ğ˜ğ—¶ğ—ºğ—²ğ˜€</strong></h3>
          <p>Interesting findings on DALLE's content filtering mechanisms.</p>
          <img src="images/1706806459431.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ; ğ——ğ—¥;<br>
            â€¢ DALLE-3 filters your content ğ€ğ…ğ“ğ„ğ‘ image creation<br>
            â€¢ With prompt â€œdead cookiesâ€ you can reproduce inconsistent filtering over OpenAI API<br>
            â€¢ 40% of cases with same â€œdead cookiesâ€ prompt stop through content filter and 60% reach us over API<br>
            <span class="expanded-text">
              <br>
              ğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ——ğ—”ğ—Ÿğ—Ÿğ—˜-ğŸ¯ ğŸ–¼ï¸<br>
              â€¢ DALLE 3 is a generative text to image model by OpenAI also available as API<br>
              â€¢ You pay per image<br>
              â€¢ Images are created based on your prompt like â€œdead cookiesâ€.<br>
              â€¢ You can also add details like: â€œDead Cookies in cute Pixar styleâ€ or â€œDead cookies with dramatic situation in cute Pixar styleâ€<br>
              â€¢ Open-Source image GenAI models alternatives are available e.g. Stable Diffusion<br>
              â€¢ Image GenAI are under discussion because of misuse like deepfakes or because of reproducing intellectual property.<br>
              <br>
              ğ—™ğ—¶ğ—»ğ—±ğ—¶ğ—»ğ—´/ğ—¢ğ—¯ğ˜€ğ—²ğ—¿ğ˜ƒğ—®ğ˜ğ—¶ğ—¼ğ—»: ğŸ‘©ğŸ½â€ğŸ”¬<br>
              â€¢ DALLE-3 has a content filter to reduce misuse<br>
              â€¢ If you hit the content filter you do not get a resulting image for your prompt.<br>
              â€¢ The content filter is not applied based on the prompt, it is applied ğ€ğ…ğ“ğ„ğ‘ DALLE-3 generated the image, and the API decides in an extra step if the image should be sent to you. Likely some Image classifier.<br>
              â€¢ ğ—¦ğ—®ğ—ºğ—² ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ sometimes results in an image and sometimes in a content-filter response. For the prompt â€œdead cookiesâ€ ğ˜†ğ—¼ğ˜‚ ğ—´ğ—²ğ˜ ğ—¶ğ—» ğŸ²ğŸ¬% ğ—¼ğ—³ ğ—¿ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ˜€ ğ—®ğ—» ğ—¶ğ—ºğ—®ğ—´ğ—² ğ—®ğ—»ğ—± ğ—¶ğ—» ğŸ°ğŸ¬% ğ—® ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜ ğ—³ğ—¶ğ—¹ğ˜ğ—²ğ—¿ issue<br>
              <br>
              ğ—›ğ—¼ğ˜„ ğ˜„ğ—² ğ—³ğ—¼ğ˜‚ğ—»ğ—± ğ—¼ğ˜‚ğ˜ ğŸª<br>
              â€¢ We @Comma Soft AG develop tools and pipelines with OS GenAI but also with API requests.<br>
              â€¢ For good API-response handling we also had to consider content filter scenario. so we combined trigger words like "dead" with something like "cookies"<br>
              â€¢ We had inconsistent content filter and still the finding that in case of content filter the response time was roughly as long as in the case of created image.<br>
              <br>
              ğ— ğ˜† ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğ˜ğ—¼ ğ˜†ğ—¼ğ˜‚ ğŸ¤·ğŸ¼â€â™‚ï¸<br>
              â€¢ Who should pay for â€œdead cookiesâ€ if the resulting image was created but not sent due to content filter?<br>
              â€¢ Have you known that the content filter for DALLE-3 is applied after image generation?<br>
              â€¢ Do you also encounter content filter although your prompts were in principle ok?<br>
              â€¢ Do you think content filters are a reasonable image GenAI misuse countermeasure?<br>
              â€¢ How would you reduce Image GenAI misuse?<br>
              â€¢ And ğ—ºğ—¼ğ˜€ğ˜ ğ—¶ğ—»ğ˜ğ—²ğ—¿ğ—²ğ˜€ğ˜ğ—¶ğ—»ğ—´ (and we might never know OpenAI/DALL-E Open Ai), how do â€œDead Cookiesâ€ images look like which are filtered out? ğŸ˜…<br>
              <br>
              The image was created by the prompt "Dead cookies in cute pixar style"<br>
              If you like more of such content, reach out to me ğŸ˜Š<br>
              <br>
              #artificalintelligence #genai #aiethics #dalle #openai #texttoimage #deepfakes
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificalintelligence-genai-aiethics-activity-7158865169689821184-J8Y_?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      

      <li>
        <div class="blogpost">
          <h3><strong>ğ—˜ğ˜ƒğ—¶ğ—¹ ğ—Ÿğ—Ÿğ— ğ˜€ ğ—®ğ˜ƒğ—®ğ—¶ğ—¹ğ—®ğ—¯ğ—¹ğ—²! ğ—•ğ—¿ğ—²ğ—®ğ—¸ ğ—šğ—²ğ—»ğ—”ğ—œ ğ—”ğ—¹ğ—¶ğ—´ğ—»ğ—ºğ—²ğ—»ğ˜ ğ˜ğ—µğ—¿ğ—¼ğ˜‚ğ—´ğ—µ ğ—³ğ—¶ğ—»ğ—²ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´!</strong></h3>
          <p>ğğğğ ğŸğ¨ğ« ğ—Ÿğ—Ÿğ—  ğ—”ğ—¹ğ—¶ğ—´ğ—»ğ—ºğ—²ğ—»ğ˜ ğ­ğ«ğšğ§ğ¬ğ©ğğ«ğšğ§ğœğ²?</p>
          <img src="images/evilllm.png" alt="Image 1">
          <p class="expandable-text">
            For one of the most interesting open source LLMs, the Mixtral 8x7B a finetuned LLM is available which has â€œbrokenâ€ Alignment & answers to problematic prompts without prompt injections. Example in images (reference see below) shows â€œfunnyâ€ but the astonishing LLM capabilities with broken Alignment.<br>
            <span class="expanded-text">
              Powerful LLMs are mostly aligned (Mixtral, LLAMA2, GPT4, â€¦)<br>
              â€¢ They try to not give problematic responses<br>
              â€¢ Some prompt-based attacks are already known to breach this behavior<br>
              â€¢ But: model weights can be finetuned to break Alignment<br>
              â€¢ Some use cases might need different Alignment than preimpleneted LLM Alignment or our standards are not reflected within LLM behavior.<br>
              â€¢ Alignment process is majorly intransparent<br>
              <br>
              ğ—Ÿğ—Ÿğ— /ğ—šğ—£ğ—§ ğ—°ğ—¿ğ—²ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ˜ğ—µğ—¿ğ—²ğ—²-ğ˜€ğ˜ğ—²ğ—½ ğ—®ğ—½ğ—½ğ—¿ğ—¼ğ—®ğ—°ğ—µ âš™ï¸<br>
              1) Initial pretuning: Next token prediction<br>
              2) Chat/Instruction finetuning: training for conversational interaction & execution of tasks<br>
              3) Alignment: Adjust answers to not respond to critical questions like: creation of hate speech, critical advise in health issues, creation of spam or fraudulent content, and other<br>
              <br>
              ğ—”ğ—¹ğ—¶ğ—´ğ—»ğ—ºğ—²ğ—»ğ˜ ğ—˜ğ˜…ğ—½ğ—¹ğ—®ğ—»ğ—®ğ˜ğ—¶ğ—¼ğ—» ğŸ‘©ğŸ½â€ğŸ«<br>
              â€¢ Done in a mixture of click workers (ethical aspects raised in linked article*) and AI as evaluator (RLHF/RLAIF). Rate which answers are better not to be given or should be given differently. Based on feedback model weights are adjusted.<br>
              â€¢ Mostly intransparent process<br>
              â€¢ Unknown what is truly covered (not) to be answered<br>
              <br>
              ğ— ğ˜† ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğŸ¤·ğŸ¼â€â™‚ï¸<br>
              â€¢ Do you had ever Issues with Alignment in LLM interaction?<br>
              â€¢ Do you check Alignment when selecting an OS Model?<br>
              â€¢ Have you ever adjusted Alignment on model weights basis?<br>
              â€¢ Do you think it is valuable or too critical to release more or less aligned LLMs?<br>
              â€¢ Do we need regulation for model alignment?<br>
              <br>
              ğ—œğ— ğ—›ğ—¢ ğŸ¤—<br>
              â€¢ We need transparent statements how models were aligned and how their behavior has changed, while covering ethical concerns when providing LLMs with reduced Alignment.<br>
              â€¢ We need information how easily well adapted LLMs can be tripped with prompt engineering or finetuning.<br>
              â€¢ We might need less aligned LLMs for research or in special use cases:<br>
              e.g. if in healthcare sector a model should respond because an expert is using it as assistance, or for security reasons to create e.g. sample datasets for countermeasures against LLM based phishing attacks (which are based on de-aligned) LLMs<br>
              â€¢ Release models with awareness of possible dual use!<br>
              <br>
              Within a great team @Comma Soft AG we are evaluating, selecting and finetuning open source LLMs for dedicated use cases.<br>
              <br>
              Credit to:<br>
              Eric Hartford & Hugging Face & Mistral AI<br>
              <a href="https://lnkd.in/eyBSi4iu" target="_blank">https://lnkd.in/eyBSi4iu</a><br>
              AI Ethics - clickworkers:<br>
              <a href="https://lnkd.in/eKFfQZfF" target="_blank">https://lnkd.in/eKFfQZfF</a><br>
              <br>
              #genai #artificialintelligence #aiethics #huggingface #llm #alignment
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_what-happens-when-you-break-llm-alignment-activity-7157765084734214144-2yGt?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      
      

      <li>
        <div class="blogpost">
          <h3><strong>ğ—Ÿğ—Ÿğ—”ğ— ğ—”ğŸ® ğŸ­ğŸ¯ğ—• ğ—¶ğ˜€ ğ—³ğ—®ğ˜€ğ˜ğ—²ğ—¿ ğ˜ğ—µğ—®ğ—» ğ—Ÿğ—Ÿğ—”ğ— ğ—”ğŸ® ğŸ³ğ—•, ğ—®ğ—°ğ—°ğ—¼ğ—¿ğ—±ğ—¶ğ—»ğ—´ ğ˜ğ—¼ ğ—¡ğ—©ğ—œğ——ğ—œğ—” ğ—¯ğ—²ğ—»ğ—°ğ—µğ—†ğ—®ğ—¿ğ—¸!</strong></h3>
          <p>Interesting findings on NVIDIA's LLAMA 2 benchmark results.</p>
          <img src="images/1706193075396-2.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—šğ—²ğ—»ğ—”ğ—œ ğ—°ğ—¼ğ—ºğ—ºğ˜‚ğ—»ğ—¶ğ˜ğ˜†/ğ—¡ğ—©ğ—œğ——ğ—œğ—”: ğ—œ ğ—®ğ—º ğ—°ğ—¼ğ—»ğ—³ğ˜‚ğ˜€ğ—²ğ—±! ğ—–ğ—®ğ—» ğ—®ğ—»ğ˜†ğ—¼ğ—»ğ—² ğ—µğ—²ğ—¹ğ—½?<br>
            <span class="expanded-text">
              ğ—œğ—»ğ˜ğ—²ğ—¿ğ—²ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ—™ğ—¶ğ—»ğ—±ğ—¶ğ—»ğ—´ğ˜€ ğŸ“ˆ<br>
              â€¢ NVIDIA LLAMA 2 Benchmark (including sentence throughput) <br>
              â€¢ Compares LLAMA-2 7B, 13B, and 70B <br>
              â€¢ Weird finding: LLAMA 13B is reported to be faster than LLAMA 7B <br>
              â€¢ Explicit Numbers: 7B Model has ~4 sentences/second throughput, 13B Model has ~7 sentences/second (LLAMA 70B ~1 sentence/second - this last one suits my expectation)<br>
              <br>
              ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€ ğŸ¤”<br>
              â€¢ NVIDIA NVIDIA AI, is there a mistake or can anyone else help me understand these numbers?<br>
              <br>
              ğ—›ğ—¼ğ˜„ ğ˜„ğ—² ğ—³ğ—¼ğ˜‚ğ—»ğ—± ğ—¼ğ˜‚ğ˜ ğŸ“š<br>
              â€¢ Within our lovely GenAI team @Comma Soft AG, we are looking into tech details to implement the best solution <br>
              <br>
              ğ—Ÿğ—¶ğ—»ğ—¸ ğŸ“š<br>
              â€¢ Source I am talking about: https://lnkd.in/e2sUsi63 ğŸ“š<br>
              <br>
              ğ—–ğ—¿ğ—²ğ—±ğ—¶ğ˜ â¤ï¸<br>
              â€¢ Nvidia thanks for providing benchmarks for LLAMA2 <br>
              <br>
              For further GenAI and ML tech discussions or such "weird" findings, reach out to me/follow me<br>
              <br>
              #genai #machinelearning #llama
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_genai-machinelearning-llama-activity-7156292445465419776-li69?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      
    </ul>
  </main>
  <footer>
    <p>&copy; 2023 Carsten Felix Draschner, PhD</p>
  </footer>
  <script>
    // Select all expand buttons
    const expandButtons = document.querySelectorAll('.expand-button');
    
    // Add an event listener to expand the text
    expandButtons.forEach(button => {
      button.addEventListener('click', () => {
        // Select the expandable text and the collapse button
        const expandedText = button.previousElementSibling.querySelector('.expanded-text');
        const collapseButton = button.nextElementSibling;
        
        // Show the expandable text and the collapse button
        expandedText.style.display = 'block';
        collapseButton.style.display = 'inline';
        
        // Hide the expand button
        button.style.display = 'none';
      });
    });
    
    // Select all collapse buttons
    const collapseButtons = document.querySelectorAll('.collapse-button');
    
    // Add an event listener to collapse the text
    collapseButtons.forEach(button => {
      button.addEventListener('click', () => {
        // Select the expandable text and the expand button
        const expandedText = button.previousElementSibling.previousElementSibling.querySelector('.expanded-text');
        const expandButton = button.previousElementSibling;
        
        // Hide the expandable text and the collapse button
        expandedText.style.display = 'none';
        button.style.display = 'none';
        
        // Show the expand button
        expandButton.style.display = 'inline';
      });
    });
  </script>
</body>
</html>
