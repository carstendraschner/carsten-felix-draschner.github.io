<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Blog - Carsten Felix Draschner, PhD</title>
  <link rel="stylesheet" href="../assets/css/styles.css">
  <style>
    /* Collapsible text */
    .expanded-text {
      display: none; /* Initially hide the text */
    }

    /* Subtle buttons */
    .expand-button, .collapse-button, .linkedin-button {
      background-color: #f0f0f0; /* Light background color matching white background */
      color: #333; /* Dark text color for better contrast */
      border: 1px solid #ccc; /* Subtle border */
      padding: 5px 10px;
      cursor: pointer;
      margin-top: 10px;
      font-size: 14px;
      display: inline-block;
    }

    .expand-button:hover, .collapse-button:hover, .linkedin-button:hover {
      background-color: #e0e0e0; /* Slightly darker on hover */
    }

    /* Center blogposts and images */
    .blogpost {
      width: 100%;
      max-width: 600px;
      margin: 0 auto;
    }

    .blogpost img {
      width: 100%; /* Image should take the full width of the container */
      height: auto;
    }

    .blogpost-content {
      text-align: left; /* Left-align the text */
    }

    .blogpost-separator {
      margin: 40px 0;
      text-align: center;
    }

    .blogpost-separator hr {
      border: 0;
      height: 1px;
      background: #ccc;
      width: 80%;
    }

    /* Remove list styling */
    ul {
      list-style-type: none; /* Remove bullets/numbers */
      padding: 0; /* Remove default padding */
    }

    /* Responsive styles */
    @media only screen and (max-width: 600px) {
      .blogpost {
        width: 100%;
        padding: 0 10px; /* Add some padding for smaller screens */
      }
      
      .expand-button, .collapse-button, .linkedin-button {
        font-size: 12px; /* Adjust button font size for smaller screens */
        padding: 5px 8px;
      }
    }
  </style>
</head>
<body>
  <header>
    <h1>Carsten Felix Draschner, PhD</h1>
    <nav>
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="index.html">Blog</a></li>
        <li><a href="../research.html">Research</a></li>
        <li><a href="../projects/index.html">Projects</a></li>
        <li><a href="../cv.html">CV</a></li>
        <li><a href="../contact.html">Contact</a></li>
      </ul>
    </nav>
  </header>
  <main>
    <h2 style="text-align: center;">Blog</h2>
    <ul>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>How Far Are We Really from Reasoning within GenAI? My Perspective in GPT-o1 times and Possible Ways to Get There ğŸ¤–ğŸ§ </strong></h3>
          <p>Exploring the current state of reasoning in GenAI, the challenges faced, and potential approaches to achieving true reasoning capabilities.</p>
          <img src="images/1728978009753.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Reasoning is key for transforming AI into AGI<br>
            â€¢ Current AI models show significant weaknesses in simple tasks<br>
            â€¢ Optimism about LLMs learning math and reasoning<br>
            â€¢ Proposed method for teaching math skills to LLMs<br>
            â€¢ Questions and call for collaboration<br>
            <span class="expanded-text">
              <strong>Reasoning is a Key Ability to Transform AI into AGI</strong><br>
              â€¢ In Reflection AI, reasoning has been unmasked. ğŸ¦¸ğŸ¼â€â™‚ï¸<br>
              â€¢ OpenAIs GPT-o1 behavior could be reproduced by silent chain-of-thought (CoT) with other models. ğŸ”„<br>
              â€¢ Reasoning is still more of a marketing promise than an actual capability. ğŸ“ˆ<br>
              â€¢ For example, we observe significant weaknesses in simple tasks. âš ï¸<br>
              â€¢ Simple multiplication is still a challenge for many models. âœ–ï¸<br>
              â€¢ As long as these fundamental issues remain unresolved, we are far from achieving true reasoning capabilities. ğŸš§<br>
              <br>
              <strong>Why I Believe LLMs Should Be Able to Learn Math and Reasoning</strong><br>
              â€¢ There are varying opinions on whether reasoning is achievable with current AI. ğŸ¤”<br>
              â€¢ I am fundamentally optimistic, although I acknowledge the recent criticisms from Yann LeCun and older from Emily M. Bender. ğŸ¦œ<br>
              â€¢ I have outlined my vision for reasoning in the comments section. âœï¸<br>
              <br>
              <strong>How to Teach Math Skills as a First Step</strong><br>
              â€¢ Let's focus on multiplication as a starting point. ğŸ”¢<br>
              â€¢ Generate pairs of numbers of different sizes. âœ–ï¸<br>
              â€¢ Split the data into training, testing, and validation sets. ğŸ“Š<br>
              â€¢ Train a language model (LLM) using a tokenizer that only includes numbers 0-9, 'x' for multiplication, and 'EOS' for the end of the sequence. ğŸ“<br>
              â€¢ Evaluate which model size yields the best results on the test set. ğŸ†<br>
              <br>
              <strong>What Do You Think?</strong><br>
              â€¢ Could such an experimental setup help evaluate the computational skills of LLMs/transformer architectures? ğŸ§ª<br>
              â€¢ Do you agree that tokens representing individual numbers are crucial for computation, rather than embedding multiple digits like "007" in a single token? ğŸ”<br>
              â€¢ Have any of you tried something similar, or would you like to collaborate on this research project? ğŸ¤<br>
              <br>
              For more content like this, follow me â¤ï¸ If you enjoyed this post, please leave a like ğŸ‘ or share your thoughts and ideas in the comments. ğŸ’¬ Image first seen in a post from Yuntian Deng. Looking forward to discuss this with my lovely colleagues at Comma Soft AG within our next AI-Coffee-Hour ğŸ¤“
              <br>
              #GenAI #Reasoning #LLM #SimpleMath
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_genai-reasoning-llm-activity-7251859387621814272-DxY6?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Stop adding Languages to LLMs! The Potential Drawbacks of Training Multilingual Large Language Models (LLMs) for Performance and Sustainability!</strong></h3>
          <p>Exploring the downsides of creating multilingual LLMs and their impact on performance and resource utilization.</p>
          <img src="images/1727692358797.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Challenges of building multilingual LLMs<br>
            â€¢ Inefficiencies in token usage and context length<br>
            â€¢ Increased hardware costs and reduced token training<br>
            â€¢ Weighing multilingual models against language-specific models<br>
            <span class="expanded-text">
              <strong>Building a Multi-Lang-LLM ğŸ› ï¸ğŸ£</strong><br>
              â€¢ When pretraining LLMs, one of the key decisions is which data to include.<br>
              â€¢ This choice also influences the selection of the tokenizer, which optimizes the number of tokens for the texts used.<br>
              â€¢ As a result, different characters and character sequences are mapped in the tokenizers for each language.<br>
              <br>
              <strong>When you finally use such a LLM in only a subset of available languages you face following problems ğŸ‡ªğŸ‡ºğŸ¤–</strong><br>
              â€¢ Inefficient token usage: If the model is only used for one or two languages, many tokens may be rarely or never needed, leading to shorter token sequences in the required language.<br>
              â€¢ Limited context length: LLMs have a limited context length, measured in tokens, which can result in more expensive inference as the model scales linearly to quadratically with prompt length.<br>
              â€¢ Increased hardware costs: This can lead to higher hardware costs and omissions.<br>
              â€¢ Reduced relevant token training: With a multilingual model, fewer relevant tokens and token sequences may have been seen and trained in the required languages.<br>
              <br>
              <strong>The Trade-Off: Multilingual Models vs. Language-Specific Models ğŸ’°ğŸ“Š</strong><br>
              â€¢ We need to weigh the benefits of multilingual models against the potential drawbacks and decide whether to prioritize language coverage or risk wasting resources.<br>
              â€¢ This is particularly important when dealing with languages that are not closely related, such as those with different character sets.<br>
              <br>
              <strong>Your Opinion ğŸ¤—</strong><br>
              â€¢ What do you think?<br>
              â€¢ Have you ever chosen a model especially with reduced number of languages outside English?<br>
              â€¢ Some more details about Multi-Lang-GenAI can be found here: <a href="https://lnkd.in/edgPsdKz" target="_blank">https://lnkd.in/edgPsdKz</a><br>
              <br>
              For more content, follow me or reach out to me over DM â¤ï¸<br>
              <br>
              #artificialintelligence #genai #llm #languages
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-llm-activity-7246524467034689537-nZ-f?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Where Science Meets Innovation: My personal Highlights & Insights into the PG 2024! Do you have answers to the open Questions?</strong></h3>
          <p>Highlights and open questions from the Petersberger GesprÃ¤che (PG) 2024, covering AI, energy transition, chip technologies, and more.</p>
          <img src="images/1727085847323.jpeg" alt="PG 2024 Highlights">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ AI and consciousness discussions<br>
            â€¢ Energy transition and regulatory challenges<br>
            â€¢ Distributed chip technologies in Europe<br>
            â€¢ Generative AI in media<br>
            â€¢ Metaverse applications beyond gaming<br>
            <span class="expanded-text">
              <strong>ğŸ§  AI and Consciousness ğŸ§ </strong><br>
              Joscha Bach from Liquid AI presented on how advancements in AI could potentially redefine our understanding of consciousness. My open question for him is: How does he intend to reliably measure the achievement of AI consciousness in his efforts?<br>
              <br>
              <strong>ğŸ‘©ğŸ»â€ğŸ”¬ Scientific Chemical Hands on how to fix energy transition ğŸ‘©ğŸ»â€ğŸ”¬</strong><br>
              As chairman of Alexander von Humboldt Foundation, Robert SchlÃ¶gl delivered an impressively passionate presentation on energy tech for the climate transition and the phasing out of fossil fuels. Besides scientific derivations, he also highlighted significant regulatory issues that hinder successful implementation. I wonder: How these regulations came about, why they are justified if they are (potentially) not scientifically tenable, and how they can now be resolved?<br>
              <br>
              <strong>ğŸ‡ªğŸ‡º Efficient Distributed Chip Technologies from the heart of Europe ğŸ‡ªğŸ‡º</strong><br>
              Christian Mayr from Technische UniversitÃ¤t Dresden discussed the potential and developments possible in Dresden. He mentioned the use of clustered chip technologies to operate LLMs. One of my open questions is: How can LLMs be distributed across tens of thousands of chips while still achieving acceptable inferential latencies?<br>
              <br>
              <strong>ğŸ“° GenAI in Media, Opportunities and Risks ğŸ“°</strong><br>
              In further discussions with Sibylle Anderl from ZEIT Verlagsgruppe, we explored the potentials and risks of Generative AI in text, image, and video generation, and how reducing anonymity could potentially restore credibility. My open question: Whether there is a reliable way to recognize these outputs, given that word frequency alone might not be a proof?<br>
              <br>
              <strong>ğŸ•¹ï¸ Metaverse, not only a Playground? ğŸ•¹ï¸</strong><br>
              Nico Michels from Siemens presented the potentials of the Metaverse and digital twins. Q: I'd love to see how such models can help even small municipalities with climate adaptation models, regional impact assessments, and improvement options?<br>
              <br>
              The open questions highlight the stimulating topics the diverse guests who aim to drive positive change with a progressive mindset. I would like to thank Comma Soft AG and everyone involved for organizing this fantastic event, especially Stephan Huthmacher, whose heartfelt dedication makes this recurring, inspiring event possible. â¤ï¸<br>
              <br>
              Share your thoughts and impressions, I'll share the streams ğŸ¤—<br>
              <br>
              Hashtag#artificialintelligence Hashtag#genai Hashtag#sustainability Hashtag#petersbergergesprÃ¤che
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-sustainability-activity-7243978164564111361-12R6?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Today's Research Proposal: How to achieve "real" thinking and reasoning in GenAI, rather than just relying on a silent Chain of Thought, as seen in ReflectionAI or possibly GPT-o1?</strong></h3>
          <p>Exploring the potential for achieving true reasoning and thinking in Generative AI models beyond the current Chain of Thought methodologies.</p>
          <img src="images/1726499742573.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Current state of reasoning in models<br>
            â€¢ Possibilities for transformers to learn to think<br>
            â€¢ Customization ideas for achieving true reasoning<br>
            â€¢ Open questions and discussion points<br>
            <span class="expanded-text">
              <strong>Currently, reasoning more through silent the CoT ğŸ˜¥</strong><br>
              â€¢ There are currently models that claim to possess reasoning capabilities.<br>
              â€¢ However, this is more about the Chain of Thought and the use of special tokens.<br>
              â€¢ These enable the model to generate more text, thereby increasing the stability of its answers.<br>
              â€¢ This process of "reasoning" is not transparently/barely shown.<br>
              â€¢ I believe this is less about reasoning or thinking and more about statistical stability.<br>
              â€¢ Moreover, it is not possible to observe the reasoning process, and it is often difficult to identify when a model has gone astray.<br>
              <br>
              <strong>How could transformers learn to think? ğŸ¤¯</strong><br>
              â€¢ Why can I still imagine that it is possible to learn to think or reason with the transformer architecture?<br>
              â€¢ Due to the transformer architecture, which includes tokenization, attention, and multi-layer perceptron elements, as well as the universal function approximation hypothesis, the following can be imagined:<br>
              1ï¸âƒ£ In early layers, the network learns something akin to named entity recognition.<br>
              2ï¸âƒ£ Later layers and embedding dimensions create structures that learn propositional logic and structures similar to knowledge graphs or their embeddings.<br>
              3ï¸âƒ£ Finally, based on this, resonating and reflecting completeness of output logic components could be achieved.<br>
              <br>
              <strong>What else would I customize? ğŸ‘¨ğŸ¼â€ğŸ’» (IMHO)</strong><br>
              I would find it interesting to combine this with thought-diffusion models, making hierarchical planning possible. I have several more ideas..., reach out to me if you like to start a discussion â¤ï¸<br>
              <br>
              <strong>Questions:</strong><br>
              Are you aware of any approaches or research projects that attempt this?<br>
              What are your thoughts on that, how would you build "AGI" it?<br>
              <br>
              I had great idea exchanges on this with colleagues from Comma Soft AG, Lamarr-Institut & Smart Data Analytics.<br>
              <br>
              Let's not waste money and time on believing in marketing claims and rebranding of the word "reasoning", but let's start to think of how it could actually be achieved ğŸ¤—<br>
              <br>
              #AGI #GenAI #artificialintelligence #research
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-sustainability-activity-7243978164564111361-12R6?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Conference Summary - Some of my Highlights and Impressions from the Lamarr AI 24!</strong></h3>
          <p>An overview of the key highlights and insights gathered from the Lamarr-Institut 24 Conference.</p>
          <img src="images/1725876824236.jpeg" alt="Conference Image">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Attended Lamarr-Institut 24 Conference<br>
            â€¢ Reconnected with colleagues and made new acquaintances<br>
            â€¢ Insights from various talks on GenAI, LLMs, and AI developments<br>
            <span class="expanded-text">
              <strong>Intro ğŸ¥°</strong><br>
              â€¢ Together with my great colleague Lukas Pfahler, PhD, we attended the Lamarr-Institut 24 Conference, where research, industry, business, and politics came together.<br>
              â€¢ It was a pleasure to reconnect with many of my companions from the Lamarr-Institut and Rheinische Friedrich-Wilhelms-UniversitÃ¤t Bonn and make new acquaintances, gathering new insights and impressions.<br>
              <br>
              <strong>My Highlights ğŸ’¡</strong><br>
              â€¢ I was delighted to see my former PhD supervisor, Jens Lehmann, now Principal Scientist at Amazon AGI and Professor at Technische UniversitÃ¤t Dresden, present on how GenAI can be implemented by LLMs and Retrieval-Augmented Generation (RAG) based on Knowledge Graphs to create better retrieval and factual correctness. I'm thrilled to see how my PhD topic and current research and development continue to grow together!<br>
              â€¢ My second PhD supervisor, Stefan Wrobel from the Fraunhofer IAIS, joined with Magnus Sahlgren from AI Sweden showcased the possibilities of LLM developments in Europe and the potential for e.g. OpenGPT-X. I'm eagerly looking forward to the model and hope it will fulfill its promises of performance. It would be excellent to see this competence and open-source claim take root in DE and EU.<br>
              â€¢ Rebecca Nugent Carnegie Mellon University delivered a thought-provoking talk on how we're currently in an AI bubble, with people outside of that bubble having a mix of disinterest, reservations, and concerns on AI that need to be addressed.<br>
              <br>
              <strong>Further Highlights in Fast Forward â©</strong><br>
              â€¢ Asja Fischer from Ruhr-UniversitÃ¤t Bochum provided exciting insights into the cat-and-mouse game of identifying GenAI content.<br>
              â€¢ Milica Gasic from Heinrich-Heine-UniversitÃ¤t DÃ¼sseldorf explained how to identify uncertainty in LLM predictions (potential hallucination).<br>
              â€¢ Mehdi Ali and Max LÃ¼bbering, PhD presented a cool pitch on an open-source framework approach to train LLMs from scratch.<br>
              â€¢ We had a great exchange with David Jurgens from the University of Michigan on the topic of alignment and system prompts.<br>
              â€¢ Sebastian Thrun created a TED-like talk supporting me and others on how to address (Gen)AI advances to a broad audience.<br>
              â€¢ The startup pitch format of the Digital Logistics Award was a pleasure to watch, Great pitches!<br>
              <br>
              I'd like to extend my gratitude to Comma Soft AG for supporting us in attending this event. I would like to thank the conference hosts for organizing such a fantastic event. Through the exchange, we've verified that we're focusing on the right approaches within our GenAI approach Alan.de â¤ï¸<br>
              <br>
              #ArtificialIntelligence #GenerativeAI #Lamarr #NRW #LLM
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-generativeai-lamarr-activity-7238914800796258306-nijX?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>For more sustainability transparency in GenAI! Share your knowledge and reduce energy waste!</strong></h3>
          <p>Emphasizing the importance of transparency and shared knowledge to enhance sustainability in GenAI.</p>
          <img src="images/1725288138494.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ GenAI involves very large models and significant training efforts<br>
            â€¢ Transparency can help share emissions and reduce energy waste<br>
            â€¢ Open source models can optimize future development<br>
            <span class="expanded-text">
              <strong>GenAI and Sustainability ğŸŒ±</strong><br>
              â€¢ GenAI is implemented through very large models.<br>
              â€¢ The training can be substantial, such as the 15T tokens of LLAMA 3.1.<br>
              â€¢ However, these trainings represent only the final training and not all the attempts that led to the final model.<br>
              <br>
              <strong>How Transparency helps ğŸ“š</strong><br>
              â€¢ If the models are open or, in the best case, open source, many users and use cases can share the resulting emissions.<br>
              â€¢ It would also be interesting to see how often models are used for inference to compare the relative share of emissions.<br>
              â€¢ Open source would be important to share learnings for the next generation of models and thus reduce emissions by reusing hyperparameter optimizations.<br>
              Please share your findings and let's use our resources and energy wisely.<br>
              <br>
              <strong>Our Paper ğŸ“„</strong><br>
              In our paper â€œEthical and Sustainability Considerations for Knowledge Graph based Machine Learning,â€ we highlight which sustainability and ethical optimizations are possible when working with hardware-intensive Artificial Intelligence approaches. Link: <a href="https://lnkd.in/eJjATkTQ" target="_blank">https://lnkd.in/eJjATkTQ</a><br>
              <br>
              What are you doing to bring AI, ethics, and sustainability together? â¤ï¸<br>
              <br>
              #artificialintelligence #genai #llm #sustainability #aiethics
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_environmental-impact-of-ai-some-model-activity-7236647589096361986-9rSm?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Sustainable Air-Gapped On-Prem LLM Solution! How can we make GenAI available on almost any hardware, and how is it also available as a portable demo on our Alan Notebook</strong></h3>
          <p>Exploring the development of a full-stack GenAI LLM solution that can run on a variety of hardware configurations, including a portable demo setup.</p>
          <img src="images/1724684606019.jpeg" alt="Alan Notebook">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Developing Alan, a full-stack GenAI LLM solution<br>
            â€¢ Hosted on German hyperscaler infrastructure<br>
            â€¢ Offers a smaller version, Alan-S-LLM<br>
            â€¢ Portable demo available on Alan Notebook<br>
            <span class="expanded-text">
              <strong>What We're Working on When We Talk About Alan R&D ğŸ‘¨ğŸ¼â€ğŸ’»</strong><br>
              â€¢ In a recent post, I introduced how we're developing Alan, a full-stack GenAI LLM solution.<br>
              â€¢ We host our solution within German hyperscaler infrastructure to deal with the requirements of multiple customer tenants and our large language models, including retrieval augmented generation pipelines.<br>
              â€¢ The requirements of our strongest Alan LLM require current top-notch Nvidia GPUs (Ampere+, 80GB VRAM), but we also offer a smaller Alan-S-LLM, which still has tremendous capabilities with fewer hardware requirements.<br>
              <br>
              <strong>How to Shrink LLMs ğŸ¤–</strong><br>
              â€¢ Models are smaller in dimensions like the number of transformer layers, heads, hidden dimensions, and other hyperparameters.<br>
              â€¢ Current smaller GenAI LLMs can be designed by model distillation and model pruning, which try to keep model quality high while reducing the number of parameters.<br>
              â€¢ The reduced number of parameters reduces VRAM requirements. Fewer parameters, especially fewer transformer layers, increase the throughput and inference performance as well.<br>
              â€¢ The reduction of bits used to represent each parameter of the LLM reduces the required total GPU VRAM.<br>
              <br>
              <strong>Our Alan Demo Notebook ğŸ’»</strong><br>
              â€¢ To demonstrate that our entire tech stack is capable of running entirely air-gapped and to showcase that we're truly capable of showing this on even a portable system, we developed the Alan-Notebook.<br>
              â€¢ This notebook uses the entire tech stack, which includes all the components that offer Multi-GPU Cluster setups, handling of users, RAG pipelines, and, of course, LLM text inference.<br>
              â€¢ The model behind is our fastest and most efficient Alan-S-Model. The notebook has limited hardware capabilities, especially within the GPU (16GB, Nvidia), but can still run the entire tech stack.<br>
              <br>
              <strong>Further Reading:</strong><br>
              â€¢ LLM Model Sizes: <a href="https://shorturl.at/6ZxMq" target="_blank">https://shorturl.at/6ZxMq</a><br>
              â€¢ Alan - Our Developer Journey: <a href="https://shorturl.at/PqLdE" target="_blank">https://shorturl.at/PqLdE</a><br>
              â€¢ Full details of how infrastructure is scaled down by Dr. Laura MaaÃŸen see <a href="https://lmy.de/eicOw" target="_blank">https://lmy.de/eicOw</a><br>
              <br>
              If you're interested in how we develop an entire scalable GenAI solution and you want to see some details into our R&D, follow me on LinkedIn and reach out to us. Thanks to my wonderful teammates Dr. Laura MaaÃŸen and Lars FlÃ¶er, who made this Alan Notebook possible, and to the entire Alan Development Team Comma Soft AG for supporting this great project and product.<br>
              <br>
              #genai #onprem #llm #alan
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_genai-onprem-llm-activity-7233851548349444097-lF4N?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Combining the Hugging Face Model Platform and Knowledge Graph trend analysis over time could improve GenAI research and reduce waste of energy!</strong></h3>
          <p>Exploring the potential of leveraging knowledge graphs to analyze trends in evolving models for better GenAI research and efficiency.</p>
          <img src="images/1723625262465.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Leveraging knowledge graphs for GenAI trends<br>
            â€¢ Identifying high-performing models and best practices<br>
            â€¢ Potential for a crowd-sourced GenAI cookbook<br>
            <span class="expanded-text">
              Traversing the possible huge knowledge graph of evolving models... It would be interesting to see trends within this graph, such as which families and architectures are trending ğŸ“ˆ. I'd love to explore over time what approaches for fine-tuning, datasets, or even the number of transformer layers/heads, etc. create high-performing and efficient models ğŸŒ±. So we could create a crowd-sourced best practice GenAI cookbook â¤ï¸ğŸ‘©ğŸ½â€ğŸ³. Please let me know if you are already working on it Thomas Wolf ğŸ¤—
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_have-you-noticed-the-new-model-tree-section-activity-7229418184590737409-UoNB?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>What is the perfect approach to adjust an LLM to your GenAI use case?</strong></h3>
          <p>Exploring various methods to customize LLMs for specific GenAI use cases, ranging from simple to complex approaches.</p>
          <img src="images/modeltraining.png" alt="Model Training">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Various ways to customize LLMs for specific use cases<br>
            â€¢ Approaches vary in difficulty and complexity<br>
            â€¢ Pros and cons of different methods<br>
            â€¢ More dimensions to improve GenAI use cases<br>
            <span class="expanded-text">
              <strong>Prompt Engineering & In-Context Learning:</strong> Manually enriching the prompt with information to guide the modelâ€™s prediction into desired behavior.<br>
              <br>
              <strong>Retrieval-Augmented Generation:</strong> Automatically retrieving context that is appended to the prompt on the fly after being retrieved through vector database similarity search and reranking.<br>
              <br>
              <strong>Parameter-Efficient Finetuning:</strong> Training only a subset of the model's parameters while keeping others unchanged.<br>
              <br>
              <strong>Full Parameter Finetuning:</strong> Training all parameters.<br>
              <br>
              These approaches have pros and cons in terms of (and not limited to):<br>
              â€¢ Catastrophic Forgetting<br>
              â€¢ Memory Load<br>
              â€¢ Maximum Context-Size limits<br>
              â€¢ Hallucinations<br>
              â€¢ Response Time<br>
              <br>
              There are also more approaches and dimensions to improve GenAI Use Cases like:<br>
              â€¢ Guardrails<br>
              â€¢ Structured output<br>
              and many more.<br>
              <br>
              What are you working on, and which field are you interested in to see how we are working on it?<br>
              <br>
              We at Comma Soft AG implement these and more approaches into our own Product Alan.de and support also other GenAI Use Cases through these approaches.<br>
              <br>
              Reach out to me ğŸ¤— and follow me for more content â¤ï¸<br>
              Hashtag#artificialintelligence Hashtag#genai Hashtag#llm
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-llm-activity-7228755735776546817-cUi0?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>These results give me hope for sustainable AI ğŸŒ±</strong></h3>
          <p>I'm impressed by some of the recent advances in the field of "small" open-weight Language Models (LLMs).</p>
          <img src="images/1722508519413.jpeg" alt="Sustainable AI">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Increased documentation supports reproducibility<br>
            â€¢ Data quality improves model performance<br>
            â€¢ Model distillation reduces hardware needs<br>
            <span class="expanded-text">
              <strong>More Documentation ğŸ“š</strong><br>
              They're accompanied by increased documentation, as seen with efforts from Apple and Meta, which support reproducibility and reduce wasted effort and energy on less promising pre-training and fine-tuning iterations.<br>
              <br>
              <strong>Data Quality as Chance ğŸ”</strong><br>
              They demonstrate that a focus on data quality can improve model performance, as evidenced by Phi-3. However, it's also clear that the total number of tokens contributes to improvements, as seen with LLAMA3(.1).<br>
              <br>
              <strong>Model Distillation ğŸ‘©ğŸ½â€ğŸ”¬</strong><br>
              Model Distillation has been effective in reducing total hardware requirements and inference time, which saves energy and resources. It will be interesting to see how quickly distilled models like Gemma2 can outperform 6-month-old state-of-the-art models like LLAMA2.<br>
              <br>
              I appreciate the efforts to decrease hardware and energy demands while still providing helpful model responses.<br>
              <br>
              Hashtag#artificialintelligence Hashtag#genai Hashtag#sustainableai Hashtag#llm
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_ai-google-tech-activity-7224785383237001216-HDyZ?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>LLMs: Big vs Small. Bigger is Better!? OR Let's not waste energy!?</strong></h3>
          <p>The AI community is abuzz with debates over the efficacy of large versus small language models. Both have their own merits and limitations.</p>
          <img src="images/modelsize.png" alt="Model Size">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ AI community debates model sizes<br>
            â€¢ Massive models vs. smaller, efficient models<br>
            â€¢ Insights and future predictions<br>
            â€¢ Links to further reading<br>
            <span class="expanded-text">
              <strong>âœ¨ The AI community is buzzing with discussions about model sizes:</strong><br>
              â€¢ Massive models like Mistral (8x22B), LLAMA3 (400B), and Grok (314B) are turning heads.<br>
              â€¢ Smaller yet mighty models like Phi 3, LLAMA3 (8B), and Nemo (12B) are proving their worth.<br>
              <br>
              <strong>ğŸ” Key insights:</strong><br>
              â€¢ Smaller models can compete by focusing on smarter training and pre-tuning methods.<br>
              â€¢ Benchmarks are helpful but not flawless in measuring a model's value.<br>
              â€¢ The mystery remains: why might a model with 61 transformer layers outperform one with 60?<br>
              â€¢ Balancing model architecture is crucial due to the increasing complexity as models scale up.<br>
              <br>
              <strong>ğŸ”® Future predictions:</strong><br>
              â€¢ Will we see a standard model size emerge, or will there be a variety for different model size clusters for different platforms (mobile, single GPU, multi-GPU clusters)?<br>
              <br>
              <strong>Links ğŸ”—</strong><br>
              â€¢ Reason LLAMA Model Sizes: <a href="https://lnkd.in/dHRSJXgm" target="_blank">https://lnkd.in/dHRSJXgm</a><br>
              â€¢ ALAN - GER-hosted GenAI tool developer story: <a href="https://lnkd.in/ey8aZTjB" target="_blank">https://lnkd.in/ey8aZTjB</a><br>
              â€¢ Problems with benchmarks: <a href="https://lnkd.in/dmBeQZ_j" target="_blank">https://lnkd.in/dmBeQZ_j</a><br>
              <br>
              <strong>ğŸŒ What is your experience?</strong><br>
              â€¢ Which model size do you find most practical for real-world problems?<br>
              <br>
              <strong>ğŸ“Š I'm keen to learn from you:</strong><br>
              â€¢ What size models are you working with?<br>
              â€¢ Share your preferences in this survey and let's discuss the optimal balance in model scaling.<br>
              â€¢ What do you think how big is the gpt4o turbo?<br>
              â€¢ Join the conversation and let's navigate the evolving landscape of machine learning together â¤ï¸<br>
              <br>
              At Alan (by Comma Soft AG), we recognize the need for variety:<br>
              We provide models of various sizes to align with your unique requirements for capability and speed.<br>
              <br>
              #MachineLearning #AI #ModelSize #Innovation #GenAI #SustainableAI
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_machinelearning-ai-modelsize-activity-7221169255419969536-IQnL?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>The BIGGEST RISK for GenAI? Interesting News from US court... Who owns the training data and which generations must not be generated? Impact of GitHub Copilot under the Digital Millennium Copyright Act (DMCA)</strong></h3>
          <p>Exploring the legal implications of GenAI, particularly in light of recent US court rulings and the challenges associated with training data and generated content.</p>
          <img src="images/1721046038993.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Capabilities and applications of GenAI<br>
            â€¢ Debates on the use of training data and distribution of generated content<br>
            â€¢ Legal prohibitions and discussions<br>
            â€¢ Challenges in attribution and data relevance<br>
            â€¢ Recent court rulings and their impact<br>
            <span class="expanded-text">
              <strong>GenAI</strong><br>
              With GenAI we are capable of doing amazing things<br>
              ğŸ¼ We can generate songs to our preferences<br>
              ğŸ“ Have texts written in a certain style<br>
              ğŸ¨ Paint pictures in a certain artistic style<br>
              ğŸ—£ï¸ Synthesize voices according to models<br>
              <br>
              <strong>Fields</strong><br>
              There is currently a big debate about how far:<br>
              1ï¸âƒ£ The training data should not have been used<br>
              2ï¸âƒ£ To what extent the generated "outcomes" should not be distributed<br>
              <br>
              <strong>Rules</strong><br>
              The following are prohibited or under discussion:<br>
              â€¢ The generation and reproduction of company logos<br>
              â€¢ Works of art may also no longer be created in the specific style of certain modern artists<br>
              â€¢ In the case of voice synthesis, there is a complaint as to whether the voice is too close to that of some popular actress<br>
              â€¢ In the case of text responses, content from paywall-protected news articles was generated without further reference<br>
              â€¢ GitHub code of others is reproduced without mentioning the source license<br>
              <br>
              <strong>Challenge</strong><br>
              Attribution is extremely difficult to impossible with naive transformer LLM architectures. Also, the total amount of data on the Internet is extremely relevant for the model's performance. I am curious to see how it turns out.<br>
              <br>
              <strong>Court</strong><br>
              GitHub Copilot and OpenAI at court: there was a surprising ruling for me in the area of copyright regarding Digital Millennium Copyright Act (DMCA), which might change the field of GenAI ğŸ‘©ğŸ½â€âš–ï¸<br>
              <br>
              <strong>Wondering</strong><br>
              I also wonder whether writing styles are forbidden:<br>
              "Write a dialog between two characters talking to each other like Jar Jar Binks and Yoda". Should this be allowed, is writing style like this protectable? ğŸ˜ˆ<br>
              <br>
              What do you think? Where should the copyright limit be? How do you interpret the current landscape of cases in court? Share within the comments ğŸ¤—<br>
              <br>
              Within our R&D team at Comma Soft AG, we facilitate a tech stack that supports proper referencing of used sources ğŸ¤–<br>
              <br>
              Join the conversation and follow me for more content â¤ï¸<br>
              <br>
              Hashtag#artificialintelligence Hashtag#llm Hashtag#genai Hashtag#copyright Hashtag#risk<br>
              <br>
              <a href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-llm-genai-activity-7218870221955043329-wGW0?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-llm-genai-activity-7218870221955043329-wGW0?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>GenAI, what is plagiarism? Its Impact on Science. How should it be handled? What is your perspective?</strong></h3>
          <p>Discussing the implications of GenAI on scientific work and the thin line between acceptable use and plagiarism.</p>
          <img src="images/1720445565584.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Use of GenAI in scientific work<br>
            â€¢ Acceptable vs. debatable vs. critical usage<br>
            â€¢ Questions and concerns about plagiarism<br>
            â€¢ The pressure on researchers and students<br>
            â€¢ Opportunities for better research<br>
            <span class="expanded-text">
              <strong>Current Situation ğŸ«</strong><br>
              However, I see ChatGPT being used in many researchers' work to assist in the creation of (scientific) texts. Often it is also controlled or prohibited by the controlling institutions (schools, universities, paper-venues) to use such tools.<br>
              <br>
              <strong>Sample GenAI-LLM usage I think is ok (IMHO) ğŸ¤—</strong><br>
              I wonder how the following behavior in ChatGPT should be dealt with:<br>
              â€¢ You ask Chat GPT if it would correct commas and or capitalization in an existing text as Office Suites do<br>
              â€¢ You ask for rewording from passive to active or better and more varied synonyms such as in Grammarly which is also "GenAI"<br>
              â€¢ You can have texts translated to make quotes more easily accessible in the desired language like DeepL or Google Translate.<br>
              <br>
              These are all uses of Chat-GPT that I can imagine in the context of scientific work that was previously possible without "ChatGPT" and, in my opinion, doable with other tools without having to speak of plagiarism.<br>
              <br>
              <strong>Debatable usage ğŸ¤·ğŸ¼â€â™‚ï¸</strong><br>
              â€¢ You are generating the abstract for your finished paper while correcting and polishing it manually yourself afterwards.<br>
              <br>
              <strong>More critical usage ğŸ§</strong><br>
              â€¢ Write an entire text, paper/exercise/... simply based on a very refined prompt with LLM+RAG+Response-Verification and hand it in as your own work.<br>
              <br>
              <strong>My Questions ğŸ‘¨ğŸ¼â€ğŸ“</strong><br>
              I just ask myself how I would have worked if the tools were there or how I would want to deal with these kinds of situations as a lecturer.<br>
              â€¢ What should be allowed?<br>
              â€¢ What goes too far and falls under plagiarism?<br>
              â€¢ Which use of (GenAI) tools needs to be regulated and documented and how finely tuned?<br>
              â€¢ How do you think scientific work, reviews, and publications will change for better or for worse?<br>
              â€¢ Does your institution at least clearly regulate how (GenAI-)Tool usage is possible<br>
              <br>
              <strong>Current Problem ğŸ«£</strong><br>
              I see lots of pressure on students and research scientists to hand in their work or to publish papers. So they are tempted to use available tools. Not every GenAI will imply plagiarism but regulations are not clear as far as I have seen.<br>
              <br>
              <strong>The Chance to Research ğŸ‘¨ğŸ¼â€ğŸ”¬</strong><br>
              We have the chance to use GenAI for better research: acquire good information over RAG and Vector Search, Translate texts into our mother tongue, and Rewrite in shorter or clearer language or even more accessible language.... Chat interact with papers you have questions to....<br>
              <br>
              We at Comma Soft AG develop GenAI solutions and also actively using such tools to create great solutions while being aware and careful about possible plagiarism issues.<br>
              <br>
              #genai #plagiarism #research #artificialintelligence<br>
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_genai-plagiarism-research-activity-7216090153457463298-37OX?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Adjust GenAI responses towards more ethical behavior possible through system prompts!? Do you trust in such LLM-chat prepended pamphlets?</strong></h3>
          <p>Exploring the potential and challenges of using system prompts to guide LLM behavior towards ethical outputs.</p>
          <img src="images/1719951079268.jpeg" alt="Ethical AI">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ GenAI chat interactions often include system prompts<br>
            â€¢ System prompts aim to guide ethical LLM behavior<br>
            â€¢ Challenges exist in ensuring compliance and formulation<br>
            â€¢ Questions on designing and revealing system prompts<br>
            <span class="expanded-text">
              <strong>Background ğŸ¤“</strong><br>
              â€¢ GenAI is often used via chat<br>
              â€¢ We enter prompts in the Chat UI<br>
              â€¢ Under the hood, the Chat programs usually add a system prompt<br>
              <br>
              <strong>System-Prompt ğŸ’¬</strong><br>
              â€¢ This system prompt should control the behavior of the LLM output<br>
              â€¢ The system prompt complements other behavioral optimizations such as alignment and other guardrails to improve ethical GenAI response<br>
              â€¢ The system prompt can describe behavior such as identity or desired behavior<br>
              <br>
              <strong>Problem ğŸ¥´</strong><br>
              â€¢ There is no guarantee to what extent the system prompt will be followed<br>
              â€¢ It is unclear what details should be included in the system prompt<br>
              â€¢ It is not possible to prevent the model from revealing the system prompt<br>
              â€¢ It is unclear how best to formulate the system prompt<br>
              <br>
              <strong>Questions ğŸ¤”</strong><br>
              â€¢ How do you design system prompts, and what are yours?<br>
              â€¢ Do you think system prompts help to ensure the model's behavior?<br>
              â€¢ Do you think chat UIs should not reveal the system prompt and how would you achieve this?<br>
              <br>
              <strong>Further Reading ğŸ“–</strong><br>
              â€¢ Change Alignment of LLMs: <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              â€¢ LLM Behavior: <a href="https://lnkd.in/ed52RBNe" target="_blank">https://lnkd.in/ed52RBNe</a><br>
              â€¢ My current LLM Developments: <a href="https://lnkd.in/ey8aZTjB" target="_blank">https://lnkd.in/ey8aZTjB</a><br>
              <br>
              We at Comma Soft AG develop GenAI solutions including optimizations of LLM responses through various components where also the design of system prompts plays a role.<br>
              <br>
              #artificialintelligence #responsibleai #llm #alan #aiethics
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-responsibleai-llm-activity-7214141563738750976-ywik?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Alternative to GenAI creativity? Watch and try out these fun Evolutionary Algorithms. Problem-solving without GenAI and SGD-based approaches explained!</strong></h3>
          <p>Exploring Evolutionary Algorithms as an alternative to GenAI for problem-solving, using a fun 2D vehicle race example.</p>
          <img src="images/evocars.png" alt="Evo Cars">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ There is hype around GenAI and LLMs<br>
            â€¢ Evolutionary Algorithms (EAs) offer an alternative<br>
            â€¢ A fun example of EAs using a 2D vehicle race<br>
            â€¢ Steps involved in EAs explained<br>
            <span class="expanded-text">
              <strong>Background â˜ºï¸</strong><br>
              â€¢ Within the field of AI is a GenAI and LLM Hype<br>
              â€¢ These are not always the best approach to solve a problem!<br>
              â€¢ There are interesting algorithms optimizing their behavior entirely differently: e.g., Evolutionary Algorithms (EA).<br>
              â€¢ Today I want to show you a fun EA introduction based on a 2D vehicle race "problem"<br>
              <br>
              <strong>Evolutionary algorithms (EAs) explained ğŸ‘¨ğŸ¼â€ğŸ«</strong><br>
              <strong>Problem ğŸ:</strong><br>
              â€¢ What is the best vehicle configuration to drive over a 2D surface?<br>
              <br>
              <strong>Vehicle Configuration = Genome ğŸ§¬:</strong><br>
              â€¢ Shape<br>
              â€¢ Wheel size<br>
              â€¢ Wheel position<br>
              â€¢ Wheel weight<br>
              â€¢ Chassis weight<br>
              <br>
              <strong>Key EA Steps on the vehicle race example ğŸ”¢ :</strong><br>
              1) Initialization: Generate a set of vehicles with random characteristics (genome) called population<br>
              2) Fitness Evaluation: Check how far they have got on the randomly generated surface.<br>
              3) Selection: Select a subset of the best vehicles from the population to reproduce and form the next generation, e.g., by tournament selection or rank selection.<br>
              4) Crossover (Recombination): Combine the genetic information of two or more selected vehicles to create new offspring. E.g., average two good car genomes or randomly select the information of each...<br>
              5) Mutation: Randomly modify the genetic information of some individuals in the population to introduce new variations and prevent convergence to a local optimum.<br>
              5) Replacement: Replace the least performing vehicles with new offspring generated through crossover and mutation.<br>
              6) Termination: Repeat until a stopping criterion is met: the needed distance, number of iterations, or convergence threshold.<br>
              7) Output: best vehicle, representing the near-optimal solution<br>
              <br>
              <strong>My Perspective ğŸ¤—:</strong><br>
              â€¢ Be aware that there are a multitude of possible algorithms to use<br>
              â€¢ We at Comma Soft AG do not trust any hype and select the appropriate algorithms for the respective problem<br>
              <br>
              <strong>Credit â¤ï¸</strong><br>
              â€¢ To the development team of the interactive EA vehicle race<br>
              â€¢ I'd also like to thank Hajira Jabeen and Jens Lehmann for their great support in my MT project where I worked on EA and KG for the creation of novel recipes<br>
              <br>
              <strong>Links ğŸ“–</strong><br>
              â€¢ EA vehicle race, try it out yourself: <a href="https://lnkd.in/e2Ew3e7D" target="_blank">https://lnkd.in/e2Ew3e7D</a><br>
              â€¢ Extract from my MT in a short paper: <a href="https://lnkd.in/e8VRVe_k" target="_blank">https://lnkd.in/e8VRVe_k</a><br>
              â€¢ My current GenAI work: <a href="https://lnkd.in/edNx8uKh" target="_blank">https://lnkd.in/edNx8uKh</a><br>
              <br>
              <strong>Questions</strong><br>
              â€¢ How would you solve this "race" problem? EA, GenAI, RL, ...?<br>
              â€¢ Do you want to see more non-GenAI content?<br>
              â€¢ Have you ever used EA to solve a problem?<br>
              <br>
              Stay tuned by following me here on Linked â¤ï¸
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-evolutionaryalgorithms-activity-7211741750673973248-V4Ej?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Do we need another GenAI solution? How & why we developed a full-stack GenAI LLM+RAG tool called Alan. A sneak peek at what I am currently working on.</strong></h3>
          <p>An overview of the motivations and technical aspects behind developing our own GenAI solution, Alan, at Comma Soft AG.</p>
          <img src="images/alanpdf.png" alt="Alan PDF">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Diverse GenAI solutions exist<br>
            â€¢ Unique motivations for developing our own tool<br>
            â€¢ Technical advantages of our solution<br>
            â€¢ Questions on custom development vs. wrappers<br>
            <span class="expanded-text">
              <strong>Current Landscape: ğŸï¸</strong><br>
              â€¢ There are a variety of GenAI solutions out there<br>
              â€¢ Many models from big tech companies work increasingly well for certain use cases.<br>
              â€¢ And we have launched our own solution. Why?<br>
              <br>
              <strong>Our technical perspective: ğŸ¤“</strong><br>
              â€¢ Data-Flow-Control: We want to know what happens to our data.<br>
              â€¢ Hosting: We host all our models and data on German servers.<br>
              â€¢ On-Premise: We can run our entire tech stack on-premise without any internet connection<br>
              â€¢ Our-own-Alan-Model: We optimize our models for dedicated use cases through further training (e.g., better language skills in GER)<br>
              â€¢ Knowledge-Enrichment: We decide for ourselves whether we want to make new knowledge available through our novel training approach or through our optimized RAG approach (as you might know there are plenty of options).<br>
              â€¢ Alignment: We have control over the alignment, i.e., to what extent the values and behaviors of the model match our expectations.<br>
              â€¢ SOTA: We always remain state of the art in a fusion of existing open source base models and in-house developments.<br>
              â€¢ Benchmarking: We use benchmarks and measures we trust.<br>
              <br>
              With a great team at Comma Soft AG, it's enjoyable to develop such a helpful tool â¤ï¸ In the appended PDF we show an extract of the full text story of our ALAN development journey ğŸ“–<br>
              <br>
              <strong>Questions: ğŸ¤”</strong><br>
              â€¢ How did you decide if you want to develop something yourself or use more kind of wrappers?<br>
              â€¢ How important is the management of sensitive data for you and where are your servers located?<br>
              <br>
              <strong>Literature: ğŸ“š</strong><br>
              â€¢ Here is the full Humboldt Travel Report with more background stories: <a href="https://lnkd.in/emwvpQsW" target="_blank">https://lnkd.in/emwvpQsW</a><br>
              â€¢ Adjusting Model Alignment <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              â€¢ Model Selection <a href="https://lnkd.in/duGzWugD" target="_blank">https://lnkd.in/duGzWugD</a><br>
              â€¢ Model Benchmarking <a href="https://lnkd.in/dmBeQZ_j" target="_blank">https://lnkd.in/dmBeQZ_j</a><br>
              <br>
              For more GenAI LLM content and R&D discussions, follow me on LinkedIn. If you want to try out Alan, reach out to me: <a href="http://alan.de" target="_blank">http://alan.de</a><br>
              <br>
              #artificialintelligence #llm #machinelearning #handson #genai
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_humboldt-travel-report-alan-llm-genai-activity-7208854388101005312-Xr_o?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Will we reach AGI, and if so, by transformers-based architectures? Share your perspective!</strong></h3>
          <p>Exploring the potential of transformers-based architectures in achieving Artificial General Intelligence (AGI) and the ongoing debate surrounding it.</p>
          <img src="images/agitarget.png" alt="AGI Target">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ GenAI's impact on AGI discussions<br>
            â€¢ Technical challenges with transformer-based architectures<br>
            â€¢ Optimistic yet cautious approach at Comma Soft AG<br>
            <span class="expanded-text">
              <strong>Why GenAI boosted the discussions around AGI! ğŸ¤¯</strong><br>
              â€¢ Several institutions are claiming that our GenAI-based architectures are a key step on our way to AGI.<br>
              â€¢ On the other hand, we are facing major technical issues with those systems that seem to not allow AGI by design: see stochastic parrots.<br>
              â€¢ But still, some, like Ilya Sutskever claim that transformer-architecture AIs can reach AGI-like capabilities as it does not only learn next token prediction, it learns a projection space for real-world understanding.<br>
              <br>
              <strong>Further links ğŸ“–</strong><br>
              â€¢ Critics on GenAI/LLMs: <a href="https://lnkd.in/e2JqstpW" target="_blank">https://lnkd.in/e2JqstpW</a><br>
              â€¢ Ilya (OpenAI) Interview: <a href="https://lnkd.in/etu4EKQT" target="_blank">https://lnkd.in/etu4EKQT</a><br>
              <br>
              We at Comma Soft AG are developing GenAI models being optimistic about the opportunities and seeing that GenAI is already helping in our tools and pipelines while being aware of all the current issues by design of these approaches (e.g., LLM hallucination) we are actively tackling.<br>
              <br>
              Share your perspective in the comments ğŸ¤—<br>
              For more content, follow me on LinkedIn â¤ï¸
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%AA%F0%9D%97%B6%F0%9D%97%B9%F0%9D%97%B9-%F0%9D%98%84%F0%9D%97%B2-%F0%9D%97%BF%F0%9D%97%B2%F0%9D%97%AE%F0%9D%97%B0%F0%9D%97%B5-%F0%9D%97%94%F0%9D%97%9A%F0%9D%97%9C-%F0%9D%97%AE%F0%9D%97%BB%F0%9D%97%B1-%F0%9D%97%B6%F0%9D%97%B3-activity-7206295399521730561-YPNs?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Do you differentiate AI Ethics principles between AI/ML fields like GenAI/LLMs or Knowledge Graph-based ML? How do we deal with so-called facts on the internet as training data?</strong></h3>
          <p>Exploring the nuances of AI ethics across different AI/ML fields and handling internet-based training data responsibly.</p>
          <img src="images/1717514744002.jpeg" alt="Ethics in AI">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ AI ethics principles across different AI/ML fields<br>
            â€¢ Personal background and perspective on AI ethics<br>
            â€¢ Recommendations and further reading on AI ethics<br>
            â€¢ Questions to ponder on AI ethics practices<br>
            <span class="expanded-text">
              <strong>My Background and Perspective ğŸ‘¶</strong><br>
              â€¢ AI ethics is always the same. No matter if GenAI, Computer Vision, or Knowledge Graph-based ML!<br>
              â€¢ In recent years, I had three main focus areas in the field of AI: My Ph.D. passion in Knowledge Graph-based Machine Learning, before a focus on computer vision, and most recently GenAI with a focus on text<br>
              â€¢ It was always important for me to take a holistic approach. This includes ethical and sustainability aspects in particular.<br>
              â€¢ In our paper on Ethical and Sustainability Implications for Knowledge Graph-based Machine Learning, we have elaborated on topics I also recently found in the literature on LLM AI Ethics [1,2].<br>
              â€¢ I see a chance that AI areas majorly align in the AI Ethics dimensions, so we can easily learn from each other and share best practices<br>
              <br>
              <strong>Recommendation ğŸ‘ğŸ½</strong><br>
              â€¢ At this point, I recommend a well-known but still very valid paper as an introduction to LLM ethics for those who may not yet know it or are new to the field: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell [1]<br>
              <br>
              <strong>Further Reading ğŸ“–</strong><br>
              [1] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? <a href="https://lnkd.in/eGt_UYci" target="_blank">https://lnkd.in/eGt_UYci</a><br>
              [2] Ethical and Sustainability Considerations for Knowledge Graph-based Machine Learning <a href="https://lnkd.in/eJjATkTQ" target="_blank">https://lnkd.in/eJjATkTQ</a><br>
              <br>
              I hope that all those who develop such powerful tools are aware of their responsibility, especially in the context of dual-use, to use their skills in a reflective manner.<br>
              <br>
              <strong>What is your perspective? ğŸ¤”</strong><br>
              â€¢ Do you see differences in the overarching AI Ethics depending on the AI field?<br>
              â€¢ What are your preferred AI Ethics Survey Papers?<br>
              â€¢ Where do you consider AI Ethics in your daily R&D AI work?<br>
              â€¢ What real issues could arise in ML models through the Wikipedia Chuck Norris fact (see image)?<br>
              <br>
              I appreciate having a GenAI R&D team Comma Soft AG that is aware of the AI ethical implications of our work.<br>
              For more content, follow me on LinkedIn or reach out to me over PM â¤ï¸<br>
              <br>
              #AIethics #artificialintelligence #llm #genai #knowledgegraphs
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_aiethics-artificialintelligence-llm-activity-7203778968548720643-SZNh?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>What expectations do you have regarding the values and norms of your GenAI chat assistants? Highly sensitive topic in the LLM space! My take...</strong></h3>
          <p>Exploring the ethical considerations and expectations surrounding the values and norms embedded in GenAI chat assistants.</p>
          <img src="images/1715804534412.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ LLMs generate text based on training<br>
            â€¢ Alignment and finetuning influence behavior<br>
            â€¢ Ethical considerations in different languages<br>
            â€¢ Need for a holistic view on model behavior<br>
            <span class="expanded-text">
              <strong>Background Information ğŸ¤“</strong><br>
              â€¢ LLMs are GenAI models that generate texts<br>
              â€¢ The text-generating behavior is based on the LLM's training<br>
              â€¢ Chat LLMs are trained in three phases<br>
              â€¢ Pretuning on large (filtered) text corpora<br>
              â€¢ Finetuning on, for example, instruction tuning data<br>
              â€¢ Final alignment to encourage preferred responses<br>
              <br>
              <strong>Some classic methods for adapting LLM behavior ğŸ‘®ğŸ¼</strong><br>
              â€¢ Filter certain content, for example, NSFW, from the pretuning data<br>
              â€¢ Try to induce certain behaviors that we consider appropriate through alignment<br>
              â€¢ Craft system prompts in which we try to give the Chat Agent specific character traits<br>
              <br>
              <strong>Possible Issues ğŸ«£</strong><br>
              â€¢ Would you expect different values and norms depending on the language in which you are prompting? Apart from alignment, culture and language may correlate with each other in the respective token spaces even in pretraining.<br>
              â€¢ Do LLMs map all languages and the values they contain into different or the same space?<br>
              â€¢ When using LLMs, we should not only pay attention to the performance of the models but also to the behavior (in other languages and token spaces)<br>
              â€¢ Does alignment-finetuning in one language resolve subjectively problematic behavior in other prompt languages?<br>
              â€¢ Who should decide about norms and values embedded within GenAI approaches? And do you check those details when selecting a pretuned model?<br>
              <br>
              <strong>Further Reading ğŸ“–</strong><br>
              â€¢ How to break Alignment of LLMs: <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              â€¢ Issues with LLM Benchmarks: <a href="https://lnkd.in/dmBeQZ_j" target="_blank">https://lnkd.in/dmBeQZ_j</a><br>
              â€¢ LLMs as Stochastic Parrots: <a href="https://lnkd.in/eGt_UYci" target="_blank">https://lnkd.in/eGt_UYci</a><br>
              â€¢ Ethical and Sustainability Issues of KG-based ML: <a href="https://lnkd.in/ejfb4qNC" target="_blank">https://lnkd.in/ejfb4qNC</a><br>
              <br>
              We at Comma Soft AG develop Large Language Models and GenAI pipelines with a holistic perspective on model behavior and performance. Please share your favorite papers or simply your experiences with this topic! For more AI topics outside the daily random model updates, please get in contact with me or follow me here on LinkedIn. â¤ï¸
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%AA%F0%9D%97%B5%F0%9D%97%AE%F0%9D%98%81-%F0%9D%97%B2%F0%9D%98%85%F0%9D%97%BD%F0%9D%97%B2%F0%9D%97%B0%F0%9D%98%81%F0%9D%97%AE%F0%9D%98%81%F0%9D%97%B6%F0%9D%97%BC%F0%9D%97%BB%F0%9D%98%80-%F0%9D%97%B1%F0%9D%97%BC-%F0%9D%98%86-activity-7196804101395939328-7ck-?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Be careful when you speak of Open (Source) GenAI. Why OpenAI and Meta (shouldn't) use the word Open within their GenAI efforts?</strong></h3>
          <p>Examining the implications of using the term "Open" in the context of GenAI by organizations like OpenAI and Meta.</p>
          <img src="images/1715104017864.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥; ğŸš…<br>
            â€¢ Open Source is a huge and important field in computer science and AI<br>
            â€¢ The word "Open" is used widely within the GenAI field: OpenAI, Open Source LLMs<br>
            <span class="expanded-text">
              <strong>Background of term open in OpenAI and Open (Source) LLMs ğŸ§‘ğŸ½â€ğŸ«</strong><br>
              â€¢ Open source libraries accelerate and improve the IT field including GenAI<br>
              â€¢ Many GenAI models are available on model hubs like Hugging Face<br>
              â€¢ Often those models like Meta LLAMA-3 are called open-source LLMs<br>
              â€¢ OpenAI provides the most successful LLM GPT4<br>
              â€¢ OpenAI does not release their model weights or creation details openly<br>
              â€¢ Elon Musk being a former founder of OpenAI sued them (maybe also cause of other reasons) for being less open than initially founded for<br>
              <br>
              <strong>Why OpenAI and Meta think it is okay to use the term "Open" ğŸ¤“</strong><br>
              â€¢ Sam Altman CEO of OpenAI claims in an interview that "Open" is valid as ChatGPT made GenAI/LLMs accessible to the world openly<br>
              â€¢ Meta claims open source as their LLM LLAMA-3 weights are freely downloadable<br>
              <br>
              <strong>Issues ğŸ˜“</strong><br>
              â€¢ No one could reproduce OpenAI or Meta's Models, so they are not open-source<br>
              â€¢ This missing open source and open research is an issue for reproducibility, fairness, security, democratization of tech, and environmental impact<br>
              <br>
              <strong>My Take (IMHO) ğŸ¤—</strong><br>
              â€¢ OpenAI can take credit for the hype around LLM-based GenAI as they truly opened access to this tool through a (partial) free open chat interface.<br>
              â€¢ Most impactful models on HuggingFace like Meta's LLAMA-3 should be considered being OPEN WEIGHT and not open source. Also, there are sometimes issues with its licenses: <a href="https://lnkd.in/dtStQSdn" target="_blank">https://lnkd.in/dtStQSdn</a><br>
              â€¢ Truly open source would mean code can entirely reproduce the models<br>
              â€¢ Still, these open-weight models accelerate GenAI/LLM developments<br>
              <br>
              <strong>Questions? ğŸ¤”</strong><br>
              â€¢ How do you see the possible confusion of the word "open" initially coming from open-source in GenAI efforts of OpenAI and Meta LLAMA-3 open-weight model?<br>
              â€¢ Do you use any truly open-source LLM?<br>
              <br>
              For more content, follow me or our team at Comma Soft AG on LinkedIn â¤ï¸<br>
              <br>
              #artificialintelligence #llm #opensource #llama #genai
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-llm-opensource-activity-7193667647274668033-1akX?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Thanks to the Open Source Community for all their efforts! Greetings from PyCon 2024</strong></h3>
          <p>Expressing gratitude to the open-source community and sharing experiences from PyConDE & PyData Berlin 2024.</p>
          <img src="images/1713878435179.jpeg" alt="PyCon 2024">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Trip to PyCon with colleagues<br>
            â€¢ Attended insightful talks in various AI fields<br>
            â€¢ Appreciation for open-source community<br>
            â€¢ Gratitude to all contributors and supporters<br>
            <span class="expanded-text">
              <strong>Trip to PyCon ğŸš…</strong><br>
              With my great colleagues, Dr. Laura MaaÃŸen and Sebastian SchÃ¶nnenbeck, from our Data Science and GenAI R&D teams, we are excited to attend PyConDE & PyData Berlin.<br>
              <br>
              <strong>Great Talks ğŸ¤</strong><br>
              We attended so many inspiring and insightful talks in the fields of Machine Learning, MLOps, GenAI, LLM, AI Ethics, and open-source software development.<br>
              <br>
              <strong>My Take ğŸ¤—</strong><br>
              â€¢ I greatly appreciate the efforts of the open-source development community!<br>
              â€¢ Having all these open-source tools and freely available tech education is such a privilege!<br>
              â€¢ I am so glad to be in a field where so much great material is accessible online for free.<br>
              â€¢ At conferences like PyCon, you meet the people behind the projects, understand their challenges, and get inspired by their motivation that drives their efforts<br>
              <br>
              <strong>Credit â¤ï¸</strong><br>
              â€¢ To all the Open Source Developers for their efforts in creating so many great and helpful tools.<br>
              â€¢ To all the open education supporters who create great material accessible to those eager to learn.<br>
              â€¢ To the PyCon Organising Team and all the Volunteers making this event possible.<br>
              â€¢ Thanks to Comma Soft AG for investing in R&D and funding us this trip with the opportunity to learn, discuss, and get inspired.<br>
              â€¢ And finally, thanks to Laura and Sebastian who supported me during the conference â¤ï¸â€ğŸ©¹. I needed help after knee surgery being at PyCon on crutches and equipped with a knee brace (sports can be dangerous ğŸ˜‰)<br>
              <br>
              #PyCon #opensource #artificialintelligence #freeeducation
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_pycon-pycon-pycon-activity-7188549430604636162-2rHg?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Who will take care of truly low-resource languages? A good step towards more fair GenAI LLM pricing at OpenAI for Japanese-using people!</strong></h3>
          <p>Exploring the challenges and recent developments in addressing low-resource languages within the GenAI landscape, with a focus on OpenAI's efforts for the Japanese language.</p>
          <img src="images/1713433871945.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Issues with LLMs for low-resource languages<br>
            â€¢ Major challenges with different character languages<br>
            â€¢ OpenAI's new dedicated model for Japanese<br>
            â€¢ Concerns about AI ethics and inequality<br>
            <span class="expanded-text">
              <strong>Recap ğŸ”</strong><br>
              â€¢ In a recent post, I described the issues of LLM solutions with "low resource" languages (see link below)<br>
              â€¢ Those issues are even more complex with languages using entirely different characters than the majorly used language English<br>
              <br>
              <strong>The major issues âš™ï¸</strong><br>
              â€¢ The same text needs more tokens to be represented<br>
              â€¢ More tokens need more time to be generated and are more costly e.g. in the OpenAI API pricing<br>
              â€¢ The quality of results is less good if a dedicated language is not in focus of model training.<br>
              â€¢ RAG use cases and also bigger task descriptions are more limited as fixed context size might limit the expression of details and context<br>
              <br>
              <strong>News by OpenAI ğŸ“°</strong><br>
              â€¢ Dedicated Model for Japan<br>
              â€¢ Focus on their tokens leading to improvements in quality and lowering effective costs (see link)<br>
              <br>
              <strong>My Take ğŸ¤—</strong><br>
              â€¢ I appreciate these efforts by Open AI<br>
              â€¢ We might need more such efforts towards further democratizing LLMs to different language heritages.<br>
              <br>
              <strong>My Major worry (AI Ethics) ğŸ˜“</strong><br>
              â€¢ GenAI will provide major gains in economic efficiency and productivity<br>
              â€¢ The development of competitive models needs huge financial investments.<br>
              â€¢ This leads to the fact that already rich countries with their languages will have access to those technologies and will over proportionally make use of those efficiency gains.<br>
              â€¢ It is unclear to what extent low-resource languages with different characters are part of LLM training.<br>
              â€¢ I fear that this can further improve inequality as the invested resources (electric energy, hardware production...) and its footprint will influence our whole planet but its gains once again only "our" privileged lives.<br>
              <br>
              <strong>Credit ğŸ˜</strong><br>
              â€¢ To OpenAI for tackling this "known" issue of unfair and inefficient handling Japanese Language<br>
              â€¢ To all R&D Teams driving efforts towards low resource language GenAI<br>
              <br>
              We @Comma Soft AG provide a B2B LLM as a Service with a focus on German and English Language and even though German is less complicated than other truly low-resource languages, it was already quite a challenge to reach high-quality results.<br>
              <br>
              <strong>Links ğŸ“š</strong><br>
              â€¢ Unfair Tokenizer: <a href="https://lnkd.in/edgPsdKz" target="_blank">https://lnkd.in/edgPsdKz</a><br>
              â€¢ OpenAI Japan: <a href="https://lnkd.in/eSwnMgJv" target="_blank">https://lnkd.in/eSwnMgJv</a><br>
              <br>
              Do you develop LLMs with a focus outside of the English Language?<br>
              <br>
              For more content, follow me â¤ï¸<br>
              <br>
              #artificialintelligence #genai #llm #aiethics #japan #openai
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-llm-activity-7186662551923884032-mz7m?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>What is your preferred LLM family? And do you start with an already finetuned LLM? Why you have chosen this LLM? I love to hear your perspective!</strong></h3>
          <p>Understanding the preferences and choices behind selecting specific LLM families and their finetuned variants.</p>
          <img src="images/whichmodel.png" alt="Which Model">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ GenAI for text implemented by LLMs<br>
            â€¢ Many open-source models available<br>
            â€¢ Continuous influx of new models<br>
            â€¢ Key foundation model families<br>
            â€¢ LLM-based GenAI pipelines at Comma Soft AG<br>
            <span class="expanded-text">
              <strong>Background Information âš™ï¸</strong><br>
              â€¢ GenAI for text can be implemented by LLMs<br>
              â€¢ LLMs are partially open-source available<br>
              â€¢ Day by day new models come to market or are published on platforms like Huggingface<br>
              â€¢ Many of them rely on certain Foundation model families<br>
              â€¢ Some of them exist in different sizes<br>
              â€¢ We @Comma Soft AG develop LLM based GenAI pipelines (sometimes using OS models like the ones in the survey)<br>
              <br>
              <strong>Further Reading ğŸ“š</strong><br>
              â€¢ Special infos about LLM licenses: <a href="https://lnkd.in/dtStQSdn" target="_blank">https://lnkd.in/dtStQSdn</a><br>
              â€¢ Reason for LLM sizes: <a href="https://lnkd.in/dHRSJXgm" target="_blank">https://lnkd.in/dHRSJXgm</a><br>
              â€¢ Broken alignment in LLMs: <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              â€¢ How to keep track of all these LLMs: <a href="https://lnkd.in/duGzWugD" target="_blank">https://lnkd.in/duGzWugD</a><br>
              â€¢ Problems tokenizer language capabilities: <a href="https://lnkd.in/edgPsdKz" target="_blank">https://lnkd.in/edgPsdKz</a><br>
              â€¢ Issues with LLM benchmark results: <a href="https://lnkd.in/dmBeQZ_j" target="_blank">https://lnkd.in/dmBeQZ_j</a><br>
              <br>
              If you need support with LLM R&D or simply want to chat, reach out to me and follow me for more content â¤ï¸
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%AA%F0%9D%97%B5%F0%9D%97%AE%F0%9D%98%81-%F0%9D%97%B6%F0%9D%98%80-%F0%9D%98%86%F0%9D%97%BC%F0%9D%98%82%F0%9D%97%BF-%F0%9D%97%BD%F0%9D%97%BF%F0%9D%97%B2%F0%9D%97%B3%F0%9D%97%B2%F0%9D%97%BF%F0%9D%97%BF%F0%9D%97%B2%F0%9D%97%B1-activity-7178654584138067968-rehP?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>NVIDIA Benchmark might be WRONG cause it states: You lose money AND LLM inference speed if you add more NVIDIA A100. This NVIDIA Benchmark is NOT reliable.</strong></h3>
          <p>Analyzing the reliability of NVIDIA's benchmark results and the implications for LLM inference speed and hardware investment.</p>
          <img src="images/1710888173070.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Terms and background on LLMs and inference<br>
            â€¢ Strange findings in NVIDIA's benchmark results<br>
            â€¢ Concerns about the reliability of these benchmarks<br>
            â€¢ Questions and further reading on the topic<br>
            <span class="expanded-text">
              <strong>Terms ğŸ«</strong><br>
              â€¢ LLMs are Large Language Models<br>
              â€¢ LLMs are a branch of Generative AI<br>
              â€¢ Those LLMs can be used to generate texts<br>
              â€¢ Text generation by LLMs is called Inference<br>
              â€¢ LLM Inference is faster on GPUs compared to CPUs<br>
              â€¢ Pretty common â€œLLM-GPUâ€ is the NVIDIA A100<br>
              <br>
              <strong>Background Story âš™ï¸</strong><br>
              â€¢ We @Comma Soft AG are developing LLM pipelines<br>
              â€¢ Each use case has different requirements<br>
              â€¢ Sometimes Inference Speed is more important<br>
              â€¢ More Hardware performance can/should improve Inference speed<br>
              â€¢ To check out how much you can improve with more hardware, you can look into the scaling effect to see the trade-off between Inference speed and hardware costs<br>
              <br>
              <strong>Weird Finding ğŸ¤”</strong><br>
              â€¢ NVIDIA released a benchmark (link see below)<br>
              â€¢ It compares different GPU setups: 1, 2, 4, 8 GPUs for a common open-source model inference<br>
              â€¢ It states that when you increase from 2 GPUs to 4 GPUs you get half the throughput; from 10 sentences/sec to 4.8 sentences/sec for LLAMA-2 13B<br>
              <br>
              <strong>My Take ğŸ¤—</strong><br>
              â€¢ The NVIDIA Benchmark is broken or some hiccup with copy-paste of results<br>
              â€¢ Sentences/sec is a strange measure. Why not tokens per second which is more stable<br>
              â€¢ I found another strange issue with model sizes and performance on NVIDIA GPUs in this benchmark. see this link: <a href="https://rb.gy/5l8qqp" target="_blank">https://rb.gy/5l8qqp</a><br>
              â€¢ It is a problem when you cannot trust benchmarks as this leads to reimplementing benchmarks or running them again which is a waste of resources and barely sustainable<br>
              â€¢ Benchmarks should be available open source to understand the measures and issues<br>
              <br>
              <strong>Questions ğŸ” </strong><br>
              â€¢ What do you think is the reason for this weird benchmark result?<br>
              â€¢ Do you have an idea why they measure in sentences per second and not in tokens per second?<br>
              â€¢ What are your preferred sources for benchmarks when it comes to Inference performance?<br>
              â€¢ What do you do to improve inference speed?<br>
              <br>
              <strong>Links ğŸ“–</strong><br>
              â€¢ NVIDIA AI Multi GPU Inference Benchmark: <a href="https://lnkd.in/e2sUsi63" target="_blank">https://lnkd.in/e2sUsi63</a><br>
              â€¢ LLAMA 13B faster than LLAMA 7B? <a href="https://rb.gy/5l8qqp" target="_blank">https://rb.gy/5l8qqp</a><br>
              â€¢ Mistrust in LLM Benchmarks! <a href="https://rb.gy/juw4pg" target="_blank">https://rb.gy/juw4pg</a><br>
              â€¢ Why do LLMs have sizes: 7B 13B, and 70B? <a href="https://rb.gy/zkpk5r" target="_blank">https://rb.gy/zkpk5r</a><br>
              <br>
              NVIDIA could you please fix it or comment on what was the issue/reason<br>
              <br>
              For more content, brainstorming, and discussions, follow me or reach out to me ğŸ¥°
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%A1%F0%9D%97%A9%F0%9D%97%9C%F0%9D%97%97%F0%9D%97%9C%F0%9D%97%94-%F0%9D%97%95%F0%9D%97%B2%F0%9D%97%BB%F0%9D%97%B0%F0%9D%97%B5%F0%9D%97%BA%F0%9D%97%AE%F0%9D%97%BF%F0%9D%97%B8-%F0%9D%97%BA%F0%9D%97%B6%F0%9D%97%B4%F0%9D%97%B5%F0%9D%98%81-activity-7176105937240231936-KzN2?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Too many LLMs?! How to keep track with all the Open Source Models? Identify the finetuned-masked LLMs and its position within the GenAI landscape!</strong></h3>
          <p>Navigating the complex landscape of GenAI models can be challenging, but it's crucial to understand the foundational and finetuned models to make informed decisions.</p>
          <img src="images/1710277843754.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ The GenAI landscape is crowded with many models <br>
            â€¢ Keeping track of innovations and true effects is hard <br>
            â€¢ Transparency issues with many so-called "open-source" models <br>
            â€¢ Recommendations for navigating this landscape<br>
            <span class="expanded-text">
              <strong>The current LLM landscape ğŸ¤–</strong><br>
              â€¢ Big GenAI LLM competition<br>
              â€¢ Lots of proprietary and open-source models<br>
              â€¢ Many Competitors: Big players like Google, Meta, and Microsoft; rising companies like Mistral and OpenAI, but also smaller institutions with foundation models or finetuned models<br>
              â€¢ Several release their model parameters openly on platforms like Huggingface<br>
              â€¢ Number of GenAI models are 500k+ (HF total), 60k+ (HF text generation)<br>
              <br>
              <strong>Challenges with number of LLMs ğŸ¤¯</strong><br>
              â€¢ Difficult to keep track with so many models<br>
              â€¢ Most innovations are only documented with blog posts or arxiv papers<br>
              â€¢ True innovations and effects are barely reported<br>
              â€¢ Model leaderboards are flooded with finetuned models<br>
              â€¢ Not always clear which and why model architecture and training data are combined<br>
              <br>
              <strong>My problem with current landscape ğŸ¤¦ğŸ¼â€â™‚ï¸</strong><br>
              â€¢ Many claim to provide open-source models but do not disclose how those are constructed. What are the true training data and initial training datasets for pretuning, instruction tuning, and alignment?<br>
              â€¢ Which training hyperparameter setup was used and why<br>
              â€¢ Why does a specific model have exactly this combination of parameters like: number transformer layers, number heads, hidden-size, intermediate size<br>
              <br>
              <strong>My recommendation ğŸ¤—</strong><br>
              â€¢ Keep calm and check what is the true source foundation model<br>
              â€¢ Understand the architecture: e.g. more or less all of those are autoregressive Decoder-Only multilayer transformer networks<br>
              â€¢ Some introduce a novel architecture ideas like MoE (Mixture of Experts)<br>
              â€¢ Do your research, demystify LLM-landscape, and implement benchmarks you trust as we do @Comma Soft AG!<br>
              <br>
              <strong>Further reading ğŸ“š</strong><br>
              â€¢ LLAMA is not truly open source! <a href="https://rb.gy/8z8t3m" target="_blank">https://rb.gy/8z8t3m</a><br>
              â€¢ Alignment can be destroyed by finetuning! <a href="https://rb.gy/ehf7s9" target="_blank">https://rb.gy/ehf7s9</a><br>
              â€¢ Why are LLMs 7, 13, 70B large? <a href="https://rb.gy/zkpk5r" target="_blank">https://rb.gy/zkpk5r</a><br>
              â€¢ My take on â€œWe have beaten ChatGPTâ€ <a href="https://rb.gy/juw4pg" target="_blank">https://rb.gy/juw4pg</a><br>
              â€¢ LMSYS Chatbot Arena <a href="https://rb.gy/f2ptbb" target="_blank">https://rb.gy/f2ptbb</a><br>
              <br>
              <strong>Credit â¤ï¸</strong><br>
              â€¢ R&D teams providing truly open-source models<br>
              â€¢ Accessible research contributions explaining effects of LLM optimization techniques<br>
              â€¢ LMSYS Chatbot Arena as an alternative to Open LLM leaderboard with focus on the original foundation models<br>
              <br>
              How do you find the most promising OS foundation model for your UC?<br>
              <br>
              For more content, brainstorming and discussions, follow me or reach out to me ğŸ¥°
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_%F0%9D%97%A7%F0%9D%97%BC%F0%9D%97%BC-%F0%9D%97%BA%F0%9D%97%AE%F0%9D%97%BB%F0%9D%98%86-%F0%9D%97%9F%F0%9D%97%9F%F0%9D%97%A0%F0%9D%98%80-%F0%9D%97%9B%F0%9D%97%BC-%F0%9D%98%81%F0%9D%97%BC-%F0%9D%97%B8-activity-7173581553552318464-6w1Z?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Be careful when you are using LLAMA-2! Legal risks & Sustainability Implications due to LLAMA-2 is (NOT) Open Source.</strong></h3>
          <p>Important considerations regarding LLAMA-2's legal and sustainability implications.</p>
          <img src="images/1709552210761-2.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ LLAMA-2's legal and sustainability challenges<br>
            â€¢ Not truly open-source according to OSD<br>
            â€¢ Technical implications of its license<br>
            â€¢ Meta's restrictions and their broader impact<br>
            <span class="expanded-text">
              <strong>What is LLAMA-2? ğŸ¦™</strong><br>
              â€¢ LLAMA 2 is a Large Language Model by Meta<br>
              â€¢ Thanks to Meta its weights are openly available over e.g. Hugging Face<br>
              â€¢ Meta claims to provide LLAMAs as open-source Models<br>
              â€¢ LLAMA-2 is under the LLAMA license which has some restrictions [1]<br>
              <br>
              <strong>Why is LLAMA NOT Open Source? [2] âŒ</strong><br>
              â€¢ Open Source means software under a license aligned with Open Source Definition (OSD)<br>
              â€¢ This includes no discrimination against persons or groups or fields of endeavor (OSD points 5 and 6)<br>
              â€¢ Metaâ€™s license puts restrictions for commercial use (paragraph 2)<br>
              â€¢ Meta also restricts the use of the model and software for certain purposes (the Acceptable Use Policy)<br>
              <br>
              <strong>Technical Implications! ğŸ‘¨ğŸ¼â€ğŸ’»</strong><br>
              IMHO, From a technical perspective the license statement $1V is another major challenge: â€œYou will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof)â€. I interpret this as a barrier if you use this model in combination with other LLMs in workflows like:<br>
              â€¢ AI as a judge in benchmark or evaluation pipelines<br>
              â€¢ Reinforcement Learning with AI Feedback - RLAIF<br>
              â€¢ Data Synthesis pipelines<br>
              â€¢ Model Distillation<br>
              <br>
              <strong>Meta creates issues with the â€œopen sourceâ€ LLAMA-2 ğŸ˜¢</strong><br>
              â€¢ We will not reach true democratization of LLM application when usage is limited like this<br>
              â€¢ Those licenses can create confusion about what is open source in general, what is truly allowed, and what is not with those LLMs<br>
              â€¢ If we can barely reuse those pre-tuned LLMs we waste a lot of energy/resources spent on pretraining on more or less the same internet text corpus. ğŸŒ±<br>
              â€¢ And we still do not know how it is truly pre-trained, finetuned, and aligned. Iâ€™d appreciate more transparency here!<br>
              <br>
              All of those statements are a part of our R&D journey @Comma Soft AG [3,4] developing LLM tools aligned with such challenging special licenses. I want to bring up the discussion about LLM usage and the meaning of open source and I am NOT giving legal advice.<br>
              <br>
              What do you think about Meta's LLAMA-2 license? Will they change it in LLAMA-3? And what implications do you derive from this license?<br>
              <br>
              <strong>Reading list ğŸ“–</strong><br>
              [1] <a href="https://lnkd.in/e9bM43p9" target="_blank">LLAMA-2 License</a><br>
              [2] <a href="https://shorturl.at/jmow3" target="_blank">Metaâ€™s LLaMa 2 license is not Open Source</a><br>
              [3] <a href="https://shorturl.at/nwzHW" target="_blank">Weird LLAMA benchmark</a><br>
              [4] <a href="https://shorturl.at/nvFZ6" target="_blank">Reasons for LLAMA sizes</a><br>
              <br>
              #artificialintelligence #machinelearning #llm #aiethics
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-machinelearning-llm-activity-7170381681563021312-_A5j?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>The major players in GenAI are facing challenges with their Generative AIs. GenAI capabilities and security issues related to LLMs Tools â€¢ 37C3 Presentation</strong></h3>
          <p>Challenges and security issues in GenAI and LLMs, highlighted at 37C3.</p>
          <img src="images/1708966127693.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ GenAI has immense capabilities<br>
            â€¢ Ethical and secure GenAI pipelines are crucial<br>
            â€¢ 37C3 presentation on security issues and exploitations<br>
            â€¢ Categories of threats and challenges in GenAI<br>
            <span class="expanded-text">
              <strong>Introduction ğŸ“–</strong><br>
              GenAI has immense capabilities and can support a variety of processes. As developers, we have the responsibility to build ethical and secure GenAI pipelines. A noteworthy but easily overlooked presentation at the 37C3 event highlighted how tools from leading companies like OpenAI, Microsoft, and Google have been exploited through "Indirect Prompt Injections".<br>
              <br>
              <strong>Threat Categories âš ï¸</strong><br>
              â€¢ Model Issues: Bias, offensive/dangerous responses, hallucinations, backdoored models.<br>
              â€¢ User as the attacker: Direct Prompt Injection, Print/Overwrite System Instructions Do-Anything-Now, Denial of Service.<br>
              â€¢ Indirect Prompt Injections: AI Injection, Scams, Data Exfiltration, Plugin Request Forgery.<br>
              <br>
              <strong>Challenges ğŸ’ªğŸ¼</strong><br>
              The development of security and safeguards for GenAI is still a "work in progress" and consists of various components ğŸ‘®ğŸ¼<br>
              â€¢ Alignment is a fine-tuning approach aimed at training model behaviors to respond securely to critical prompts. Learn more here: <a href="https://lnkd.in/eWS-VZCD" target="_blank">https://lnkd.in/eWS-VZCD</a><br>
              â€¢ In Image GenAI, for instance, you can use a post-processing content filter. Check out this peculiar content filter behavior here: <a href="https://lnkd.in/eRaX9JuT" target="_blank">https://lnkd.in/eRaX9JuT</a><br>
              â€¢ An interesting set of additional threats and potential countermeasures were recently presented at the 37C3 event, focusing on Indirect Prompt Injections.<br>
              <br>
              <strong>Credits & Links â¤ï¸</strong><br>
              â€¢ If you're interested in this topic, especially Indirect Prompt Injections, I highly recommend the 37C3 presentation. Watch the video on YouTube: <a href="https://lnkd.in/enjMee6S" target="_blank">https://lnkd.in/enjMee6S</a><br>
              â€¢ A huge thanks to everyone supporting the security of these GenAI tools and for their tireless efforts in educating and presenting their findings for free. Special thanks to Johann Rehberger<br>
              â€¢ As part of an exceptional team at @Comma Soft AG, we're developing pipelines with a holistic perspective on GenAI.<br>
              <br>
              #generativeai #llm #aisecurity #aiethics #37c3
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_generativeai-llm-aisecurity-activity-7167923469148475392-plqf?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Not prompting in English?... You have Problems!! LLM Language Barriers â€¢ Democratizing GenAI and fair pricing</strong></h3>
          <p>Exploring the challenges of using Generative AI with languages other than English and the implications for cost and performance.</p>
          <img src="images/1708345591968-4.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Tokenizers and their role in LLMs<br>
            â€¢ Challenges of non-English prompts<br>
            â€¢ Efficiency and fairness in GenAI<br>
            â€¢ Recommendations for LLM pipelines<br>
            <span class="expanded-text">
              <strong>Tokenizer âœ‚ï¸</strong><br>
              â€¢ Generative AI Large Language Models process tokens<br>
              â€¢ Tokenizers are a preprocessing step of LLMs and cut prompts into subwords called tokens<br>
              â€¢ Tokenizers are optimized to efficiently compress text into a limited number of tokens<br>
              â€¢ Tokenizers are optimized based on large text corpora<br>
              â€¢ LLM pipeline hardware load grows based on number of tokens<br>
              â€¢ OpenAI GPT API charges per token<br>
              <br>
              <strong>My findings & my takes ğŸ˜°</strong><br>
              â€¢ You do not prompt in English... You have Problems!<br>
              â€¢ You have to pay more if you are communicating in a language other than English as the tokenizer creates more tokens (see image)<br>
              â€¢ You'll likely get worse results from LLMs since they haven't seen as much text in your target language during pretraining<br>
              â€¢ You can fit less non-English text into an LLM, which has limited context length, meaning you can't solve similarly complex tasks with same LLM<br>
              â€¢ Using LLMs in German I am still quite lucky since German is reasonably well represented in most training data corpora and due to the relative similarity between German and English the difference in token decompositions is less pronounced. However, there are many languages (in particular those not using the Latin alphabet) for which the effects are much larger.<br>
              <br>
              <strong>IMHO ğŸ¤—</strong><br>
              â€¢ We will gather extraordinary efficiency gains through generative AI but we need to be careful that these gains are available to truly all people including those not using GenAI in English<br>
              â€¢ If you are building your own LLM pipeline as we do at Comma Soft AG, check out how the tokenizer and language model interact with your language. This might affect the costs or max context length of your text<br>
              â€¢ As the pricing per token is based on the technical costs to process a certain text, I can understand it from that perspectiveâ€¦ But it is unpleasant as you pay more for the same prompt while likely getting worse results and having more limitations in prompt complexity e.g. when using RAG insertions<br>
              â€¢ Possibly LLMaaS could provide differently trained LLMs focused on regions and languages yielding better tokenizers and performance for lower resource languages<br>
              <br>
              <strong>Related Topics ğŸ“š</strong><br>
              1. Who should decide about alignment? especially when we speak about regional LLMs: <a href="https://shorturl.at/dwGIQ" target="_blank">https://shorturl.at/dwGIQ</a><br>
              2. What is fair pricing for content-filtered GenAI results: <a href="https://shorturl.at/aCF16" target="_blank">https://shorturl.at/aCF16</a><br>
              3. What is the best LLM for you: <a href="https://shorturl.at/bmuP8" target="_blank">https://shorturl.at/bmuP8</a><br>
              4. Try out Open AI tokenizer: <a href="https://lnkd.in/eKhrP7qi" target="_blank">https://lnkd.in/eKhrP7qi</a><br>
              <br>
              Hashtag#generativeai Hashtag#artificialintelligence Hashtag#machinelearning Hashtag#aiethics<br>
              <br>
              For more content, brainstorming, and discussions, follow me or reach out to me â¤ï¸
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_genrativeai-artificialintelligence-machinelearning-activity-7165320754878730242-CICO?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Ever wondered about open source LLM sizes: 7B, 13B, 70B?</strong></h3>
          <p>Where do those model sizes come from?... My findings!</p>
          <img src="images/1707838124577-3.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Open-source LLM alternatives to AIaaS<br>
            â€¢ Hugging Face as a source for open-source models<br>
            â€¢ Many models are finetuned variants<br>
            â€¢ Bigger models imply slower inference & higher costs<br>
            â€¢ Different use cases require different model capabilities<br>
            â€¢ Questioning the parameter step sizes of models<br>
            <span class="expanded-text">
              <strong>Background ğŸ“</strong><br>
              â€¢ As an alternative to AIaaS like ChatGPT, you can interact with LLMs based on open-source models<br>
              â€¢ A good source for open-source models is the Hugging Face model library.<br>
              â€¢ Many models are finetuned variants of existing models like the Meta LLAMA-2 is available as 7B, 13B, 70B.<br>
              â€¢ Inference runs faster on GPUs like NVIDIA V/A/H100.<br>
              â€¢ Bigger models: slower inference & have higher (environmental) costs while bigger LLMs mostly outperform smaller models in benchmark-tasks.<br>
              â€¢ Within multiple use cases, I select the best fitting OS model at @Comma Soft AG use cases.<br>
              â€¢ Different use cases request different model capabilities including model â€œknowledgeâ€ or inference speed.<br>
              â€¢ I was wondering why many models follow the parameter â€œstep sizesâ€ 7B, 13B, and 70B.<br>
              <br>
              <strong>My Findings ğŸ”</strong><br>
              â€¢ Many models are finetuned versions of LLAMA-2 as this was a high-performing open-source LLM available within a â€œmostlyâ€ attractive OS license.<br>
              â€¢ In most cases, model finetuning does not change the number of parameters.<br>
              â€¢ LLAMA paper states it provides LLMs: [...] "that achieve the best possible performance at various inference budgets" [...]<br>
              â€¢ Common hardware is 16GB or 80GB of VRAM. Usually, you have one or two of those GPUs within a system.<br>
              â€¢ Models are by default available as 16bit representation which leads to 2byte per parameter.<br>
              â€¢ To run a model, you need space for parameters and a bit remaining for your batch. So 7B fits on 1x 16GB GPU, 13B fits on 2x 16GB GPUs, (the Lab-leaked LLAMA-1 with 35B fits on 1x 80GB GPU) and the 70B LLAMA-2 model runs on 2x 80GB GPUs.<br>
              <br>
              <strong>IMHO ğŸ¤—</strong><br>
              â€¢ I am looking forward to how 4bit quantization like GPTQ or AWQ changes the model sizes as you might also fit a roughly 145B quantized model on a single A100 with 80GB.<br>
              â€¢ Some use cases might need fewer model parameters but bigger batches, longer max context length, or faster inference which means fewer parameters or fewer deep networks.<br>
              â€¢ Consider smaller models especially because of the environmental costs if performance is sufficient.<br>
              â€¢ I am wondering if there is a true reason for how the parameters are combined within the architecture, the numbers partially feel randomly picked like 80 transformer layers for LLAMA-2 70B vs 40 of 13B version.<br>
              <br>
              <strong>Questions ğŸ¤”</strong><br>
              â€¢ What is your preferred model(-family)?<br>
              â€¢ Do you use your models as plain or quantized versions?<br>
              â€¢ Do you think the model architectures of finetuned context window, hidden size, intermediate size, number heads, and transformer layers are well chosen that build the total needed VRAM volume?<br>
              <br>
              Follow me for more content â¤ï¸<br>
              <br>
              #artificialintelligence #genai #machinelearning #llm
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificialintelligence-genai-maschinelearning-activity-7163192285034172416-AItW?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>We've beaten GPT4! â€¦ is a sentence which starts to annoy me. About Mistrust in LLM Evaluation.</strong></h3>
          <p>Benchmark contamination in LLMs? How to evaluate GenAI?!</p>
          <img src="images/1707147429256-3.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ News is flooded with LLMs being â€œbetterâ€ than GPT4 <br>
            â€¢ LLM Evaluation is Difficult <br>
            â€¢ Benchmark Contamination is a serious issue <br>
            â€¢ Build your own use case specific benchmarks<br>
            <span class="expanded-text">
              <strong>Why do we need LLM benchmarks? ğŸ“Š</strong><br>
              â€¢ Many use cases can be solved by GenAI more specifically by LLMs<br>
              â€¢ Many LLMs are available as AIaaS or as an open-source model<br>
              â€¢ At some point, you need to select a specific model for your dedicated use case<br>
              â€¢ News is flooded by a multitude of models that are better than some reference LLM like OpenAIs GPT4<br>
              <br>
              <strong>Why is LLM Evaluation difficult? ğŸ‘©ğŸ½â€ğŸ”¬</strong><br>
              â€¢ When we speak about leaderboards and benchmarks, we look into specific types of tasks.<br>
              â€¢ Those tasks need to be â€œeasily measurableâ€ as LLM might generate arbitrary texts.<br>
              â€¢ e.g. MMLU is simply a multiple choice and looks if first generated character is A-E.<br>
              â€¢ Other Benchmarks use e.g. another LLM as judge (which is expensive) and also fuzzy.<br>
              â€¢ It is almost impossible to measure if an LLM learned the benchmark data by heart in its pretuning/finetuning stage to get on top of the leaderboard.<br>
              <br>
              <strong>My Takes ğŸ”</strong><br>
              â€¢ Leaderboards are only a starting point for model selection.<br>
              â€¢ GenAI approach selection is a multidimensional problem.<br>
              â€¢ Develop a use-case-specific evaluation framework e.g. does the generated code run/match unit tests, is secure and fast<br>
              â€¢ For most of my use cases I barely care if the model can solve English multiple choice questions by simply evaluating if the first character is an A, B, C, D, or E like in MMLU.<br>
              â€¢ Already simple throughput benchmarks seem to have their issues. see: <a href="https://rb.gy/5l8qqp" target="_blank">https://rb.gy/5l8qqp</a><br>
              <br>
              <strong>Extract of my hands-on criteria ğŸ‘¨ğŸ¼â€ğŸ’»</strong><br>
              â€¢ Under which license is the model available and does the license allow my intended usage?<br>
              â€¢ What do we know about retuning especially regarding: multi-language support, instruction tuning, alignment, and context length?<br>
              â€¢ What hardware requirements/costs do we face, and which throughput can we provide? e.g. 13B vs 8x7B vs 70B ...<br>
              <br>
              <strong>Credit â¤ï¸</strong><br>
              â€¢ To Hugging Face and other platforms for providing LLM Leaderboards and easily accessible models<br>
              â€¢ To OpenAI with its GPT-4 as reference established to be beaten<br>
              â€¢ To all benchmark creators and researchers supporting transparent and reliable GenAI evaluation<br>
              <br>
              <strong>My Questions ?</strong><br>
              â€¢ What are the criteria you look at?<br>
              â€¢ What are the best benchmarks for you & why do you trust those?<br>
              â€¢ Do you create your own benchmarks as we do Comma Soft AG<br>
              <br>
              #generativeai #artificialintelligence #llm #machinelearning #benchmark
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_generativeai-artificialintelligence-llm-activity-7160295294906101761-koWS?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>DALLE has surprising guardrails. Your image is not filtered based on your prompt. "Dead cookies" may be generated ...sometimes</strong></h3>
          <p>Interesting findings on DALLE's content filtering mechanisms.</p>
          <img src="images/1706806459431.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ DALLE-3 filters your content AFTER image creation<br>
            â€¢ With prompt â€œdead cookiesâ€ you can reproduce inconsistent filtering over OpenAI API<br>
            â€¢ 40% of cases with same â€œdead cookiesâ€ prompt stop through content filter and 60% reach us over API<br>
            <span class="expanded-text">
              <strong>What is DALLE-3 ğŸ–¼ï¸</strong><br>
              â€¢ DALLE 3 is a generative text-to-image model by OpenAI also available as API<br>
              â€¢ You pay per image<br>
              â€¢ Images are created based on your prompt like â€œdead cookiesâ€.<br>
              â€¢ You can also add details like: â€œDead Cookies in cute Pixar styleâ€ or â€œDead cookies with dramatic situation in cute Pixar styleâ€<br>
              â€¢ Open-Source image GenAI models alternatives are available e.g., Stable Diffusion<br>
              â€¢ Image GenAI are under discussion because of misuse like deepfakes or reproducing intellectual property.<br>
              <br>
              <strong>Finding/Observation: ğŸ‘©ğŸ½â€ğŸ”¬</strong><br>
              â€¢ DALLE-3 has a content filter to reduce misuse<br>
              â€¢ If you hit the content filter you do not get a resulting image for your prompt.<br>
              â€¢ The content filter is not applied based on the prompt, it is applied AFTER DALLE-3 generated the image, and the API decides in an extra step if the image should be sent to you. Likely some Image classifier.<br>
              â€¢ Same prompt sometimes results in an image and sometimes in a content-filter response. For the prompt â€œdead cookiesâ€ you get in 60% of requests an image and in 40% a content filter issue<br>
              <br>
              <strong>How we found out ğŸª</strong><br>
              â€¢ We @Comma Soft AG develop tools and pipelines with OS GenAI but also with API requests.<br>
              â€¢ For good API-response handling, we also had to consider content filter scenarios. So we combined trigger words like "dead" with something like "cookies"<br>
              â€¢ We had inconsistent content filter and still the finding that in case of content filter the response time was roughly as long as in the case of created image.<br>
              <br>
              <strong>My Questions to you ğŸ¤·ğŸ¼â€â™‚ï¸</strong><br>
              â€¢ Who should pay for â€œdead cookiesâ€ if the resulting image was created but not sent due to content filter?<br>
              â€¢ Have you known that the content filter for DALLE-3 is applied after image generation?<br>
              â€¢ Do you also encounter content filter although your prompts were in principle ok?<br>
              â€¢ Do you think content filters are a reasonable image GenAI misuse countermeasure?<br>
              â€¢ How would you reduce Image GenAI misuse?<br>
              â€¢ And most interesting (and we might never know OpenAI/DALL-E Open Ai), how do â€œDead Cookiesâ€ images look like which are filtered out? ğŸ˜…<br>
              <br>
              The image was created by the prompt "Dead cookies in cute pixar style"<br>
              If you like more of such content, reach out to me ğŸ˜Š<br>
              <br>
              #artificalintelligence #genai #aiethics #dalle #openai #texttoimage #deepfakes
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_artificalintelligence-genai-aiethics-activity-7158865169689821184-J8Y_?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>Evil LLMs available! Break GenAI Alignment through finetuning!</strong></h3>
          <p>Need for LLM Alignment transparency?</p>
          <img src="images/evilllm.png" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ Powerful LLMs are mostly aligned<br>
            â€¢ Alignment can be broken through finetuning<br>
            â€¢ Need for transparency in alignment processes<br>
            â€¢ Questions about alignment in LLMs<br>
            <span class="expanded-text">
              For one of the most interesting open source LLMs, the Mixtral 8x7B a finetuned LLM is available which has â€œbrokenâ€ Alignment & answers to problematic prompts without prompt injections. Example in images (reference see below) shows â€œfunnyâ€ but the astonishing LLM capabilities with broken Alignment.<br>
              <br>
              <strong>Powerful LLMs are mostly aligned (Mixtral, LLAMA2, GPT4, â€¦)</strong><br>
              â€¢ They try to not give problematic responses<br>
              â€¢ Some prompt-based attacks are already known to breach this behavior<br>
              â€¢ But: model weights can be finetuned to break Alignment<br>
              â€¢ Some use cases might need different Alignment than pre-implemented LLM Alignment or our standards are not reflected within LLM behavior.<br>
              â€¢ Alignment process is majorly intransparent<br>
              <br>
              <strong>LLM/GPT creation three-step approach âš™ï¸</strong><br>
              1) Initial pretuning: Next token prediction<br>
              2) Chat/Instruction finetuning: training for conversational interaction & execution of tasks<br>
              3) Alignment: Adjust answers to not respond to critical questions like: creation of hate speech, critical advice in health issues, creation of spam or fraudulent content, and other<br>
              <br>
              <strong>Alignment Explanation ğŸ‘©ğŸ½â€ğŸ«</strong><br>
              â€¢ Done in a mixture of click workers (ethical aspects raised in linked article*) and AI as evaluator (RLHF/RLAIF). Rate which answers are better not to be given or should be given differently. Based on feedback model weights are adjusted.<br>
              â€¢ Mostly intransparent process<br>
              â€¢ Unknown what is truly covered (not) to be answered<br>
              <br>
              <strong>My Questions ğŸ¤·ğŸ¼â€â™‚ï¸</strong><br>
              â€¢ Do you had ever issues with Alignment in LLM interaction?<br>
              â€¢ Do you check Alignment when selecting an OS Model?<br>
              â€¢ Have you ever adjusted Alignment on model weights basis?<br>
              â€¢ Do you think it is valuable or too critical to release more or less aligned LLMs?<br>
              â€¢ Do we need regulation for model alignment?<br>
              <br>
              <strong>IMHO ğŸ¤—</strong><br>
              â€¢ We need transparent statements on how models were aligned and how their behavior has changed, while covering ethical concerns when providing LLMs with reduced Alignment.<br>
              â€¢ We need information on how easily well-adapted LLMs can be tripped with prompt engineering or finetuning.<br>
              â€¢ We might need less aligned LLMs for research or in special use cases:<br>
              e.g. if in the healthcare sector a model should respond because an expert is using it as assistance, or for security reasons to create e.g. sample datasets for countermeasures against LLM-based phishing attacks (which are based on de-aligned) LLMs<br>
              â€¢ Release models with awareness of possible dual use!<br>
              <br>
              Within a great team @Comma Soft AG we are evaluating, selecting and finetuning open source LLMs for dedicated use cases.<br>
              <br>
              Credit to:<br>
              Eric Hartford & Hugging Face & Mistral AI<br>
              <a href="https://lnkd.in/eyBSi4iu" target="_blank">https://lnkd.in/eyBSi4iu</a><br>
              AI Ethics - clickworkers:<br>
              <a href="https://lnkd.in/eKFfQZfF" target="_blank">https://lnkd.in/eKFfQZfF</a><br>
              <br>
              #genai #artificialintelligence #aiethics #huggingface #llm #alignment
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_what-happens-when-you-break-llm-alignment-activity-7157765084734214144-2yGt?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>      
      <div class="blogpost-separator"><hr></div>
      <li>
        <div class="blogpost">
          <h3><strong>LLAMA2 13B is faster than LLAMA2 7B, according to NVIDIA benchmark!</strong></h3>
          <p>Interesting findings on NVIDIA's LLAMA 2 benchmark results.</p>
          <img src="images/1706193075396-2.jpeg" alt="Image 1">
          <p class="expandable-text">
            ğ—§ğ—Ÿ;ğ——ğ—¥ â±ï¸<br>
            â€¢ NVIDIA LLAMA 2 Benchmark results<br>
            â€¢ LLAMA 13B reported faster than LLAMA 7B<br>
            â€¢ Questions about the accuracy of these findings<br>
            â€¢ Seeking community insights<br>
            <span class="expanded-text">
              <strong>GenAI community/NVIDIA: I am confused! Can anyone help?</strong><br>
              <strong>Interesting Findings ğŸ“ˆ</strong><br>
              â€¢ NVIDIA LLAMA 2 Benchmark (including sentence throughput)<br>
              â€¢ Compares LLAMA-2 7B, 13B, and 70B<br>
              â€¢ Weird finding: LLAMA 13B is reported to be faster than LLAMA 7B<br>
              â€¢ Explicit Numbers: 7B Model has ~4 sentences/second throughput, 13B Model has ~7 sentences/second (LLAMA 70B ~1 sentence/second - this last one suits my expectation)<br>
              <br>
              <strong>Questions ğŸ¤”</strong><br>
              â€¢ NVIDIA NVIDIA AI, is there a mistake or can anyone else help me understand these numbers?<br>
              <br>
              <strong>How we found out ğŸ“š</strong><br>
              â€¢ Within our lovely GenAI team @Comma Soft AG, we are looking into tech details to implement the best solution<br>
              <br>
              <strong>Link ğŸ“š</strong><br>
              â€¢ Source I am talking about: <a href="https://lnkd.in/e2sUsi63" target="_blank">https://lnkd.in/e2sUsi63</a> ğŸ“š<br>
              <br>
              <strong>Credit â¤ï¸</strong><br>
              â€¢ Nvidia thanks for providing benchmarks for LLAMA2<br>
              <br>
              For further GenAI and ML tech discussions or such "weird" findings, reach out to me/follow me<br>
              <br>
              #genai #machinelearning #llama
            </span>
          </p>
          <button class="expand-button">Read more</button>
          <button class="collapse-button" style="display: none;">Read less</button>
          <a class="linkedin-button" href="https://www.linkedin.com/posts/carsten-draschner_genai-machinelearning-llama-activity-7156292445465419776-li69?utm_source=share&utm_medium=member_desktop" target="_blank">LinkedIn Post</a>
        </div>
      </li>
      
      
    </ul>
  </main>
  <footer>
    <p>&copy; 2023 Carsten Felix Draschner, PhD</p>
  </footer>
  <script>
    // Select all expand buttons
    const expandButtons = document.querySelectorAll('.expand-button');
    
    // Add an event listener to expand the text
    expandButtons.forEach(button => {
      button.addEventListener('click', () => {
        // Select the expandable text and the collapse button
        const expandedText = button.previousElementSibling.querySelector('.expanded-text');
        const collapseButton = button.nextElementSibling;
        
        // Show the expandable text and the collapse button
        expandedText.style.display = 'block';
        collapseButton.style.display = 'inline';
        
        // Hide the expand button
        button.style.display = 'none';
      });
    });
    
    // Select all collapse buttons
    const collapseButtons = document.querySelectorAll('.collapse-button');
    
    // Add an event listener to collapse the text
    collapseButtons.forEach(button => {
      button.addEventListener('click', () => {
        // Select the expandable text and the expand button
        const expandedText = button.previousElementSibling.previousElementSibling.querySelector('.expanded-text');
        const expandButton = button.previousElementSibling;
        
        // Hide the expandable text and the collapse button
        expandedText.style.display = 'none';
        button.style.display = 'none';
        
        // Show the expand button
        expandButton.style.display = 'inline';
      });
    });
  </script>
</body>
</html>